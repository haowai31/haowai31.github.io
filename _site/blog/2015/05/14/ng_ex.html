<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
  <meta name="description" content="Readings">
  <meta name="author" content="haowai31">
  <meta name="keywords" content="NG机器学习的编程实验, Thinking and Recording, haowai31">
  <title>NG机器学习的编程实验 - Thinking and Recording</title>
  <link rel="canonical" href="http://haowai31.github.io//blog/2015/05/14/ng_ex.html">
  <link rel="icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="shortcut icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="/res/css/public.css">
  <link rel="stylesheet" href="/res/css/light.css">
  <script src="/res/js/light.js"></script>
  <script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>
</head>
<body>
  <div id="blog">
    <div class="sidebar">
<!--  <div class="profilepic">
    <a href="/"><img src="" alt="logo"></img></a>
  </div> -->
  <h1 class="title"><a href="/">Thinking and Recording</a></h1>
  <h2 class="sub-title"></h2>
  <nav id="nav">
    <ul>
    
      <li><a href="/page/timing.html"><i class="fa fa-clock-o"></i>&nbsp;时间轴</a></li>
    
      <li><a href="/page/category.html"><i class="fa fa-tags"></i>&nbsp;分类</a></li>
    
      <li><a href="/page/read.html"><i class="fa fa-book"></i>&nbsp;阅读</a></li>
    
      <li><a href="/page/about.html"><i class="fa fa-paper-plane-o"></i>&nbsp;关于</a></li>
    
    </ul>
  </nav>  
  <nav id="sub-nav">
    <a class="weibo " href="" title="新浪微博" target="_blank"><i class="fa fa-weibo"></i></a>
    <a class="github" href="https://haowai31.github.com/" title="GitHub" target="_blank"><i class="fa fa-github fa-2x"></i></a>
    <a class="rss" href="/page/feed.xml" title="RSS订阅" target="_blank"><i class="fa fa-rss"></i></a>
  </nav>
  <div id="license">
    <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" title="本站所有作品采用：&#10;知识共享《署名 非商业性使用 相同方式共享 3.0》&#10;进行许可" >
    <img alt="License" height="31" width="88" src="data:image/png;base64,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" /></a>
  </div>
</div>

    <div class="main">
    <header class="post-header">
  <h1 class="post-title"><a href="/blog/2015/05/14/ng_ex.html">NG机器学习的编程实验</a></h2>
  <p class="post-meta">
    <i class="fa fa-calendar"></i>
    2015年05月14日
    <i class="space"></i>
    <i class="fa fa-tags"></i>
    <a class="post-category" href="/page/category.html#机器学习">机器学习</a>	
  </p>
</header>
<div class="post-main">
<p><a href="#ex1">实验一：线性回归</a><br />
<a href="#ex2">实验二：逻辑回归</a><br />
<a href="#ex3">实验三：神经网络</a><br />
<a href="#ex4">实验四：神经网络反向传播算法</a><br />
<a href="#ex5">实验五：线性回归和修正欠拟合与过拟合</a><br />
<a href="#ex6">实验六：支持向量机</a></p>

<!-- more -->

<p><a name="ex1"></a></p>

<h2 id="programming-assignment-linear-regression">实验一，Programming Assignment: Linear Regression</h2>

<p>这个实验就是用来熟悉一下提交的环境，另外在学习算法的各个过程都需要填几行关键程序，包括怎么求估价函数，怎么施行阶梯算法，怎么标准化数据等等，选做实验是两种特征的训练数据。</p>

<h3 id="section">1.1 单位矩阵</h3>
<p>就是通过eye函数得一个单位矩阵。</p>

<h3 id="j">1.2 计算估价函数J</h3>
<p>（对不起我还不会写公式，回来更新）</p>

<pre><code>J = sum((X * theta - y).^2) / (2*m);
</code></pre>

<p>注意其中的.^，这是矩阵特有的操作符号，是矩阵的每一个对应的位置进行相应的运算。sum函数则是对矩阵求和。如果得出的结果是32.07就说明程序写的没问题，总之矩阵运算一定要注意的是x*y对应起来，否则如果出错还容易发现矩阵运算没有写错，如果恰好没有提示错误，得出的结果不对这种Bug会改的头破血流。</p>

<h3 id="section-1">1.3 实现阶梯下降算法</h3>
<p>（公式仍然不会写）</p>

<pre><code>theta(1) = theta(1) - alpha/m*sum(X*theta_backup-y);
theta(2) = theta(2) - alpha/m*sum((X*theta_backup-y).*X(:,2));
</code></pre>

<p>这只是单纯照着阶梯下降算法更新theta就好。值得注意的地方在上课的时候NG也说了，因为再更新theta的过程中会改变theta，所以我这里用了一个 ** theta_backup ** 对原来的theta进行了备份。这是一个特征的情况，下面还会有多种特征的简便写法（我第一次写的是循环^o^）。</p>

<p>这个实验做到这里其实已差不多了，看到最后画出的图还是很开心的，虽然感觉上跟我并没有什么关系。</p>

<h3 id="section-2">1.4 多类特征的附加实验</h3>
<p>具体步骤跟单类特征差不多，值得注意的就是上文提到的更新theta的简便写法：</p>

<pre><code>theta = theta - alpha / m * X' * (X * theta - y); 
</code></pre>

<p>X’是求X的转置矩阵。</p>

<p>这是对数据标准化的语句，话说在pdf中间直接告诉要怎么做这样真的好嘛？连需要用的函数都给了。。。</p>

<pre><code>mu = mean(X);
sigma = std(X);
X_norm = (X - repmat(mu, size(X,1) , 1))  ./ repmat(sigma,size(X,1),1);
</code></pre>

<p>mean()函数是求平均值，具体自己做做实验就知道了，我就是一边儿一边儿在shell上面测试函数的用法，实用是学习最快的方法。std()函数是求标准差，然后完全按照pdf的描述就可以写下这条语句了（当然，我这么笨的人在shell上面测试了好久。。）。</p>

<p>另外视频中也提到过，如果数据规模小的话，可以直接使用矩阵进行计算：</p>

<pre><code>theta = pinv(X'*X)*X'*y;
</code></pre>

<p>pinv()函数是求矩阵的逆，其实就是这个函数限制着这种干脆利索的方法的运行速度的。</p>

<h3 id="section-3">1.5 总结</h3>
<p>总之，第一次实验叫做 <strong>worm exercises</strong> 也是有一定道理的。其实就是给 <strong>纸上得来</strong> 和 <strong>恭行</strong> 两件事建立个联系，知道这些算法是可以实现并且实际工作的，这对于我这样的笨蛋还是很重要的，学的时候总觉得隔层纱，到真的程序运行结果砸在脸上了，就能迷糊回来了。</p>

<p><a name="ex2"></a></p>

<h2 id="logistic-regression">实验二，Logistic Regression</h2>

<p>该实验总共包含两个小实验，一个是基础的逻辑回归实验，另一个是情况稍微复杂一点的实验，需要拟合的数据不是直线，当然其实如果没想那么多，跟着pdf做的话，几乎感觉不到区别，除了最后可视化的图不太一样。</p>

<h3 id="section-4">2.1 将数据可视化</h3>

<p>可以看到，边界线大概是条直线。</p>

<h3 id="section-5">2.2 实现</h3>

<p>经过前面的实验熏陶，已经知道这个实验实现部分肯定包括CostFunction，下降率，自己对数据进行预测等等，所有需要做的就是照着公式用octave实现出来就可，再次强调 <strong>矩阵的行和列</strong> 是验算式子写的对不对的有效的方法。
sigmoid函数很简单，不过注意的是这个函数接受的参数z，是矩阵，所以使用操作符号的时候一定要注意，该用“./”就要用。
求J的CostFunction，解析起来比较复杂，还是惯例，从里到外解析，注意运算符左右的矩阵的行和列，一般不会有什么问题，具体公式如下所示：</p>

<pre><code>J = 1 / m * sum((-y).*log(sigmoid(X*theta)) - (1-y).*log(1-sigmoid(X*theta)));
</code></pre>

<p>要注意的还是诸如“.*”这些操作符号。
而相关的gradient更新的公式如下：</p>

<pre><code>grad = 1 / m * (X' * (sigmoid(X*theta) - y));
</code></pre>

<hr />
<p>其实costfunction写完之后，需要修改的文件已经差不多没了，但是任务并没有结束，有了costfunction要如何更新theta呢？
这里是使用了octave里面提供的fminunc函数，通过这个函数算出对于CostFunction来说最优的参数。使用方式如下所示：</p>

<pre><code>options = optimset('GradObj', 'on', 'MaxIter', 400);
[theta, cost] = fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);
</code></pre>

<p>GradObj参数告诉fminunc使用gradient来计算需要返回theta和cost两个值，设置MaxIter最大步数为400。后面@(t)的写法是内部调用函数，同时提供theta和上文提到的options就可以等待fminunc算出结果了。并不用自己制定下降率之类的参数，大部分工作都由fminunc做了。</p>

<h3 id="section-6">2.3 评估结果</h3>

<p>这里就是将已经得到的theta带进h函数就好，然后把所有的值在0.5以上的数据挑出来，将其y置1就好，使用find，很容易达到目的。</p>

<pre><code>tmp = sigmoid(X*theta);
finde = find(tmp&gt;=0.5);
p(finde,1) = 1;
</code></pre>

<p>然后就可以看到结果啦。</p>

<h3 id="regularized-logistic-regression">2.4 第二个实验：Regularized logistic regression</h3>

<p>整个流程和第一个实验差不多，需要注意的是costFunction中计算J函数和gradient更新，对于j=0和j&gt;0是不一样的处理方式，另外在mapFeature函数中，使用了6次的多项式对数据进行拟合，可以看其拟合的手法，值得学习。
最后还给出了过度拟合与拟合度不够的边界值的图，可以看看。</p>

<h3 id="section-7">2.5 总结</h3>

<p>整体来说，大部分程序不需要自己动手，只是部分函数自己填一下就可以了，像填空一样，但是做到现在，应该慢慢对整个体系有一些印象，回来是需要自己把整个过程都写出来的，提前做准备。</p>

<p><a name="ex3"></a></p>

<h2 id="section-8">实验三，神经网络算法</h2>

<p>这个实验使用了视频最后讲的识别手写数字作为数据输入，分别使用了多种特征的逻辑回归算法与神经网络算法进行实验，用来加强这两种算法的对比。实际运行时也会有个大概的印象，在特征较多时<strong>神经网络算法果然比逻辑回归要快</strong>。</p>

<h3 id="section-9">3.1 多重特征的逻辑回归算法</h3>

<p>这一小节就是对上一次实验的复习，如果上次实验costFunction和grad写的好的话，这次可以直接拿来用。但是果然还是再做一边比较好。
然后调用fmincg函数，比之前用的fminunc在处理大规模数据方面更有效率。
但是使用方法和fminunc几乎一样。
然后根据前面训练出来的theta对手写数字进行预测，一切都差不多。需要注意的就是这里使用max函数，得出的是每行最大的值（越大越正确），需要用一个两列的矩阵接受max函数的返回值，这样，第一列返回的是最大值，第二列返回的是最大值所在的位置。第二列才是我们需要的。第几个最大，预测就是几。</p>

<h3 id="section-10">3.2 神经网络算法</h3>

<p>所有的theta已经算好，而且模型也给出了，是一个具有一个hidden层的模型，hidden层具有25个单元，输出层有10个单元。
那由于数据是有4000个特征的。所以可以预测theta1是25×401的矩阵，而theta2是10×26的矩阵。
于是有了theta之后，就可以一层一层的进行计算了，每层记得加上x0。
如下所示：</p>

<pre><code>tmpX = [ones(m,1) X];
layer1 = sigmoid(tmpX * Theta1');
tmplayer1 = [ones(m,1) layer1];
layer2 = sigmoid(tmplayer1 * Theta2');
[a,p] = max(layer2,[],2);
</code></pre>

<p>代码很简单，最后的max函数上面也讲过了，需要用一个两列的矩阵接受返回值，第二列就是我们的预测值。（他把10当作0了）</p>

<p><a name="ex4"></a></p>

<h2 id="section-11">实验四 神经网络与反向传播算法</h2>

<p>本次实验主要实现了包括反向传播算法在内的比较完整的神经网络学习算法，由于这个算法比较复杂，实现起来步骤也比较多，所以经常容易出错，甚至这个算法后面还花了很大的篇幅给了一个校验算法，可能都觉得这算法偷摸在后台跑着很不让程序员放心。
本次实验将神经网络学习算法分为了两个部分，第一是前向传播，这部分和上个实验的相关部分相近，神经网络模型也是三层，Theta1是25 × 401，Theta2是10 × 26。在之后写代码的时候会经常遇到。</p>

<h3 id="section-12">4.1 前向算法的实现</h3>

<p>具体步骤和实验三类似，但是需要注意的是，这里输出层包含了十个单元，但是给出的y只是1-10的数字，需要将y转成位图存储，具体代码如下：</p>

<pre><code>Y = [];
E = eye(num_labels);
for i=1:num_labels
	Y0 = find(y==i);
	Y(Y0,:) = repmat(E(i,:),size(Y0,1),1);
end
</code></pre>

<p>主要用了find函数与repmat函数，find函数前面有介绍，而repmat函数就是重复矩阵，所以这里又要说了，矩阵运算赛高～～</p>

<p>这里的costFunction看起来非常复杂，需要算有三个求和符号的式子，而且就算是向量算式仍然还有两个求和符号，善用sum求和函数即可，复杂但是不难。对了，记得加上正则化的参数。</p>

<p>learning部分，包括梯度导数都将在下一节介绍。</p>

<h3 id="section-13">4.2 后向传播算法的实现</h3>
<p>这一次实验的重点来了，梯度导数算起来步骤繁多，还是善用解构的方法，现处理和别的联系不大的模块，官方给的pdf也是先要处理sigmoid导数计算的小模块，pdf中已经给出了导数的式子，没事儿可以自己求个导～
将随机初始化处理了之后，就开始本次实验的重头戏，后向传播算法。官方pdf中显然也是非常不放心将步骤这么多的一个算法交给我们这些小朋友，所以步骤都给清楚了，而我在做的时候，也是老老实实重新看了一边视频，将重点重新复习了一下才开始动笔写的。好了严格按照步骤写就好。</p>

<p><strong>step 1</strong><br />
求a1，我们知道a1就是X中某一个样本特征，但是记得加上bias。由于要算偏差值和delta，所以需要再进行一边前向传播，分别算出a2，a3，z2，z3。</p>

<p><strong>step 2</strong><br />
算偏差值，err3单独算，然后按照给的公式一步一步算出err2就好。这里给出err2的计算式子。</p>

<pre><code>err_2 = (Theta2' * err_3);
err_2 = err_2(2:end) .* sigmoidGradient(z_2);
</code></pre>

<p>这里切记切记，z2是25 × 1，而err2是26 × 1，需要将bias干掉再算，当然如果在写代码的时候时刻记着现在矩阵计算的行和列，那就不会像我一样犯错误了。</p>

<p><strong>step 3</strong><br />
计算delta1和delta2。按照公式计算即可。</p>

<pre><code>delta_2 = delta_2 + (err_3)*a_2';
delta_1 = delta_1 + (err_2)*a_1';
</code></pre>

<p><strong>step 4</strong>
计算两个梯度导数，还是一样按照公式计算即可。</p>

<pre><code>Theta1_grad = 1 / m * delta_1 + lambda / m * theta1_tmp;
Theta2_grad = 1 / m * delta_2 + lambda / m * theta2_tmp;
</code></pre>

<p>其实按照步骤分解，每一步视频都讲的挺清楚，注意代码编写正确即可。</p>

<p>之后官方已经给了校验算法，自己跑一遍ex4就行。</p>

<h3 id="section-14">4.3 关于隐藏层的可视化</h3>

<p>这个额外的实验就一个问题：隐藏层究竟代表了什么？
其实咱们小时候学习的时候也是这样，学写字，大概长这个样子就是什么字儿，大概是这个样子是什么句子（还记得那个文字顺序一点儿都不影响阅读么？）。原来在看名家在争论白马非马的时候就跟着思考过这个概念，就是<strong>大概长这个样子</strong>用不是人话描述就叫模型，所谓马为天下之马而白马就这一匹而已。之所以这个算法叫神经网络算法，其实也是这样，每一个中间层可能就是对某一类事物的模型，每层递进的过程中都在一点儿一点儿完善这个模型。</p>

<p><a name="ex5"></a></p>

<h2 id="section-15">实验五 线性回归与修正过拟合与欠拟合</h2>
<p>这个实验包含了三个实验，一个是通过学习选取比较好的线性回归模型，一个是通过观察学习曲线来修正过拟合与欠拟合，另一个是通过学习选取多项式特征模型，通过学习，选取表现较好的。</p>

<h3 id="section-16">5.1 实现线性拟合</h3>
<p>就是回顾之前的线性拟合的各种算法。按照pdf上写的CostFunction和梯度选择的公式写就行了。</p>

<h3 id="section-17">5.2 学习曲线</h3>
<p>分别计算训练组和交叉验证组的误差。为了获得不同的训练组集合，这里只是要求大小不同，所以这里使用了循环来依次改变训练组集合的大小，而交叉验证组单独给出。
这里使用trainLinearReg函数得出theta，然后使用上个实验写好的linearRegCostFunction函数求出各个误差，这里lambda为0。
从得出的训练集的大小与误差的曲线图来看，上个实验得出的线性拟合模型工作的很不错。在下一个实验就会给出使用多项式系数继续进行拟合的方法，通常来说那样做会拟合的更好。</p>

<h3 id="section-18">5.3 使用多项式系数拟合</h3>
<p>如何表示多项式系数是一个问题，这里先预处理，算出所有可能的x的次方，算出的X_poly是一个m×p的矩阵，这个矩阵每一列都是一个x的次方，从x的1次方到x的p次方。在polyFeatures.m中进行计算就好。很简单。
然后次方数p根据学习选择之后，最后一个问题就是确定lambda。同样如何确定lambda呢？这里还是采用的循环的方式，题目提供了一个lambda可能的集合：{0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10}。可以看到，这就是原来讲过的lambda取值的方式，ng经常这样取，然后根据训练集与交叉验证集的方式选择出比较恰当的lambda。</p>

<h3 id="section-19">5.4 之后的事情</h3>
<p><strong>事情远没有结束。</strong>
首先要使用剩下的测试集来验证lambda是否选择适当。而test就是做这个事情的。
而前面选择训练集的时候，并没有做随机化，是依次从前多少个中取得的训练集，这样做会使得学习的结果更加局部化，只是满足局部的训练集，所以这里应该是随机化选择训练集。不过也可以理解，如果是随机化选择训练集，那么结果就没办法评分了。但是这儿还是应该随机化选择的。</p>

<p><a name="ex6"></a></p>

<h2 id="section-20">实验六 支持向量机</h2>
<p>这个实验一共包含了两个实验，一个是支持向量机的实验，包含了三个dataset，第一个是线性的后两个个是非线性的。第二个实验是在垃圾邮件分类方面应用支持向量机进行计算。</p>

<h3 id="section-21">6.1 高斯核函数</h3>
<p>实现高斯核函数，计算某点和l之间距离的时候可以通过向量进行快速计算。</p>

<h3 id="section-22">6.2 确定两个常数</h3>
<p>确定C与sigma这两个在高斯核函数中的常数。通过使用支持向量机进行训练，并对交叉验证组进行验证，在一定范围内选出比较合适的C和sigma即可。
首先，C和sigma的范围怎么算，这里提示说仍然使用从0.01依次乘以3进行选取，所以为了程序好写，则需要将可能的C和sigma放在向量中。然后就对不同的C和sigma进行排列组合，训练之后算出交叉验证组的值，选择误差最小的。
Octave里面有svm相关的函数，但是pdf中稍微提了一句，并没有告诉具体的用法，我也是在ex6中找到的，然后查了查资料，然后把这个函数写出来了，如下所示：</p>

<pre><code>model = svmTrain(X, y, C_vec(i), @(x1, x2) gaussianKernel(x1, x2, sigma_vec(j)));
</code></pre>

<p>这中间C_vec和sigma_vec两个向量就是C和sigma的取值范围。
算出model之后，需要利用model对交叉验证组进行对照，然后算出误差。这里不再贴代码。
之后选择误差最小的即可。其中可能要用到ind2sub函数，可以自行百度。</p>

<h3 id="section-23">6.3 垃圾邮件分类</h3>
<p>实验已经对邮件的预处理，可以看看中间都有什么处理，思路还是很重要的，但是这里主要讲实验。
不过其实实验也没啥讲的，在预处理之后，需要完成的就是对单个单词的比对，而且敏感词表也已经给了，如果存在这个敏感词则在一个很长很长的向量中标注一下即可。
然后万事俱备，（纳尼，我啥都没干呢！）就可以使用svm算法对垃圾邮件进行分类了。
之后给的两个附加实验倒是稍微有意思一点，一个就是用现有的model测试自己的邮件，第二个就更彻底一点儿，从SpamAssassin Public Corpus上面下到一些可以作为实验数据的东西，然后自己重新组织程序，通过processEmail和emailFeatures两个函数获得自己的X和y，然后进行训练。记得把实验数据分成训练组、交叉验证组和测试组。</p>

<p>实验一更新（2015.05.14 21：28）<br />
实验二更新（2015.05.20 09：50）<br />
实验三更新（2015.05.27 09：23）<br />
实验四更新（2015.06.09 09：09）<br />
实验五更新（2015.06.29 09：17）<br />
实验六更新（2015.07.11 15：17）</p>

</div>
<div class="share">
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
  </div>
  <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":["qzone","tsina","weixin","sqq"],"bdPic":"","bdStyle":"0","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/res/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>
</div>
<div class="pagination">

<a class="pagination-item newer" href="/blog/2015/05/12/hello.html"><i class="fa fa-arrow-left"></i>&nbsp;&nbsp;你好，旅行者～</a>


<a class="pagination-item older" href="/blog/2015/06/11/Security_Protocols.html">安全协议读书笔记（一）&nbsp;&nbsp;<i class="fa fa-arrow-right"></i></a>

</div>
<style>#ds-reset .ds-avatar, #ds-reset .ds-avatar img{border-radius:50%;}</style>
<script type="text/javascript">var duoshuoQuery = {short_name:"11780753"};(function(){var ds = document.createElement('script');ds.type = 'text/javascript';ds.async = true;ds.src = '/res/js/embed.js';ds.charset = 'UTF-8';(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);})();</script>
<div class="comments">
  <div class="ds-thread" data-title="NG机器学习的编程实验" data-thread-key="/blog/2015/05/14/ng_ex" data-url="http://haowai31.github.io//blog/2015/05/14/ng_ex.html"></div>
</div>
    <footer>Copyright&nbsp;&copy;&nbsp;2015 <a href="http://haowai31.github.io/">Thinking and Recording</a><br/><i class="fa fa-cogs" style="color:blueviolet;">&nbsp;</i>Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a></footer>
    </div>
  </div>    
  <div id="top"><a id="rocket" href="javascript:;" title="返回顶部"><i></i></a></div>
  
</body>
</html>