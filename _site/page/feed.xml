<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RSS - Thinking and Recording</title>
    <description>Thinking and Recording - </description>
    <link>http://haowai31.github.io/</link>
    <atom:link href="http://haowai31.github.io//page/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 16 Jul 2015 21:36:35 +0800</pubDate>
    <lastBuildDate>Thu, 16 Jul 2015 21:36:35 +0800</lastBuildDate>
    <generator>haowai31</generator>
    
      <item>
        <title>crest的安装与使用</title>
        <description>&lt;p&gt;crest是一款针对C语言的自动测试工具，支持深度优先（DFS）、控制流图（CFG）、一致随机（uniform random）和随机分支（random branch）四种搜索策略，使用符号执行技术，使用Yices作为约束求解器进行求解。&lt;/p&gt;

&lt;p&gt;github地址为：https://github.com/jburnim/crest&lt;/p&gt;

&lt;p&gt;下面是安装步骤和test的测试。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;安装环境&lt;/h3&gt;
&lt;p&gt;Ubuntu 14.04 32bit&lt;br /&gt;
Crest 0.1.2&lt;br /&gt;
Yices 1.0.40&lt;br /&gt;
Ocaml 3.12.1&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;安装步骤&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;安装Cmaml：由于Crest需要CIL的支持，而CIL需要Ocaml的支持，所以先安装Ocaml，在googlegroup中作者说Ocaml 4.01.0可以bugfree，否则CIL可能会停止响应，但是我直接用的Ubuntu的apt-get，安装的3.12.1，运行起来也没啥问题。&lt;/li&gt;
  &lt;li&gt;安装Yices，这个版本是特别古老的版本，Yices官网都已经给的是Yices2了，但是也提供老版本的下载，只是没有源码了，只有编译好的binary，自己按需下载，依赖包这种东西，错一个版本几乎就不能用，我不信邪，装了Yices2，结果。。我还是用了Yices1.0.40。之后按照Crest的要求，把crest/src/Makefile Yices的目录更新一下，方便编译。&lt;/li&gt;
  &lt;li&gt;安装CIL：依赖包都安装完了，现在开始安装Crest，但是之前，要先安装CIL，去Crest/CIL/目录configure一下，看看缺什么包自己安装就好了，不过一般都是ocaml-find和libgmp-dev两个包。configure通过之后再make就好了。&lt;/li&gt;
  &lt;li&gt;安装Crest：去Crest/src/目录下，make即可完成安装。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;test&quot;&gt;test实验&lt;/h3&gt;
&lt;p&gt;github上面给了一个test的运行例子，我也就是照着做了一遍，大概流程如下：
首先，测试的C程序是test目录下的uniform_test.c，关键代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if (a == 5) {
	if (b == 19) {
		if (c == 7) {
			if (d == 4) {
				fprintf(stderr, &quot;GOAL!\n&quot;);
			}
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，如果约数求解成功，应该有八个分支，并且其中一个分支应该输出“GOAL！”。然后把程序改装一下，需要进行一些代码插桩，比如将变量利用CREST的函数声明，然后输出的时候使用stderr进行输出，这样就可以看到例子了。这些事情test的代码都已经做好，具体可以看看test是怎么做的。然后使用bin目录下的crest对代码进行插桩和编译工作。使用run_crest运行已经插桩好的二进制文件，可以得出如下输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Iteration 0 (0s): covered 0 branches [0 reach funs, 0 reach branches].
Iteration 1 (0s): covered 1 branches [1 reach funs, 8 reach branches].
Iteration 2 (0s): covered 3 branches [1 reach funs, 8 reach branches].
Iteration 3 (0s): covered 5 branches [1 reach funs, 8 reach branches].
Iteration 4 (0s): covered 7 branches [1 reach funs, 8 reach branches].
GOAL!
Iteration 5 (0s): covered 8 branches [1 reach funs, 8 reach branches].
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，已经得出了8个分支，而且也输出了“GOAL！”。任务算是完成了吧。&lt;/p&gt;

&lt;p&gt;PS：这仅仅是对单个C文件的测试，据作者说还可以测试工程，等回来再行实验。&lt;/p&gt;

</description>
        <pubDate>Thu, 16 Jul 2015 00:00:00 +0800</pubDate>
        <link>http://haowai31.github.io//blog/2015/07/16/crest.html</link>
        <guid isPermaLink="true">http://haowai31.github.io//blog/2015/07/16/crest.html</guid>
        
        <category>程序分析</category>
        
      </item>
    
      <item>
        <title>iOS安全实验记录（一）：搭建环境与所需工具</title>
        <description>&lt;p&gt;会持续更新iOS相关的安全实验，具体多少更不清楚，有好的主题就继续往下做，包括但不限于以下几个方面（这是已经做过和很感兴趣的）：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;逆向某些App。&lt;/li&gt;
  &lt;li&gt;Hook与注入实验。&lt;/li&gt;
  &lt;li&gt;ROP攻击实验。&lt;/li&gt;
  &lt;li&gt;越狱开发。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;有兴趣探讨实验或提供实验主题请联系&lt;strong&gt;qinsky31@gmail.com&lt;/strong&gt;。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;今天，主要讲一下我这里搭建的环境，以及使用工具。&lt;/p&gt;

&lt;h3 id=&quot;theos&quot;&gt;Theos&lt;/h3&gt;
&lt;p&gt;这是一款越狱开发的工具包，与iOSOpenDev不同，这个工具包并不那么的&lt;strong&gt;完善&lt;/strong&gt;，而逆向工程中很多东西无法自动化，所以使用这个不怎么完善的工具包可以方便对整个逆向的过程理解更加清晰明确。&lt;/p&gt;

&lt;h3 id=&quot;ldid&quot;&gt;ldid&lt;/h3&gt;
&lt;p&gt;签名用，在MAC上放一个也可以，在iPhone上放一个也可以。&lt;/p&gt;

&lt;h3 id=&quot;cydiasubstrate&quot;&gt;CydiaSubstrate&lt;/h3&gt;
&lt;p&gt;著名的越狱开发的基础包，主要提供了三种功能，使用简单方便：Hook，Loader，Safemode。Hook和Loader都不陌生，可以对目标进程进行Hook和注入。Safemode是CydiaSubstrate引入的专为越狱开发而做的，因为Loader使用的dylib载入第三方lib的，如果第三方lib（也就是自己的lib）出错的话，会让被注入的进程崩溃。而这个safemode可以识别SIGTRAP、SIGABRT、SIGILL、SIGBUS、SIGSEGV、SIGSYS六个信号，一旦被触发就进入safemode，禁用所有第三方lib，方便调试。&lt;/p&gt;

&lt;h3 id=&quot;cycript&quot;&gt;Cycript&lt;/h3&gt;
&lt;p&gt;一个可以进行运行在MTerminal的脚本语言，很方便的就注入进程，查看某函数的运行结果，某个中间结果都非常方便。&lt;/p&gt;

&lt;h3 id=&quot;lldbdebugserver&quot;&gt;LLDB和debugserver&lt;/h3&gt;
&lt;p&gt;由于笔者研究方向与LLVM相关，所以老老实实的用lldb进行调试了。&lt;/p&gt;

&lt;h3 id=&quot;openssh&quot;&gt;OpenSSH&lt;/h3&gt;
&lt;p&gt;方便手机远程调试。&lt;/p&gt;

&lt;h3 id=&quot;usbmuxd&quot;&gt;usbmuxd&lt;/h3&gt;
&lt;p&gt;一个将USB协议抽象成为TCP协议的服务，可以使用USB插口进行调试。&lt;/p&gt;

&lt;h3 id=&quot;mterminal&quot;&gt;MTerminal&lt;/h3&gt;
&lt;p&gt;iOS上的终端。&lt;/p&gt;

&lt;h3 id=&quot;syslogd&quot;&gt;syslogd&lt;/h3&gt;
&lt;p&gt;记录系统日志。进行越狱开发的时候syslog是不可或缺的帮手。&lt;/p&gt;

&lt;h3 id=&quot;otool&quot;&gt;otool&lt;/h3&gt;
&lt;p&gt;不用安装，可以查看程序依赖的动态库。&lt;/p&gt;

&lt;h2 id=&quot;hello-world&quot;&gt;Hello World&lt;/h2&gt;
&lt;p&gt;按照惯例，写一个helloworld的程序，放在手机里执行作为开始。要在MAC上编译一个可以在iOS上执行的C程序，需要编译器，也需要指定SDK，下面分别进行介绍。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;编译器&lt;/h3&gt;
&lt;p&gt;原来交叉编译环境都是arm-开头，中间应该有关键字llvm-gcc，我找了半天，毛都没有，后面去Xcode更新日志上看到，从Xcode开始编译器都指定为clang，不再使用llvm-gcc，这一头包。后来在StackOverflow上面找到了一种方法，就是执行命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;xcrun -f --sdk iphoneos clang
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;xcrun是Xcode中command line tool的工具，这个命令可以找到Xcode使用的sdk包的地址。而iphoneos则是指定的平台，这样就找到了clang的位置。不过，这里clang是一个通用的编译器，需要指定平台和SDK包才行。&lt;/p&gt;

&lt;h3 id=&quot;sdk&quot;&gt;SDK&lt;/h3&gt;
&lt;p&gt;SDK包的位置倒是好找一点，直接find搜所有的SDKs找到相应SDK的位置即可。我要在iPhone执行我的helloworld，那么就选iPhone的SDK，这里SDK的版本是8.4。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;编译&lt;/h3&gt;
&lt;p&gt;运行命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;clang --target=arm64-apple-darwin14.4.0 -isysroot $SDKs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这中间target是指定平台，这里我用的iPhone6p，所以平台是arm64，isysroot指定的是SDK的文件，这里是我自己设的变量，需要改成自己的目录。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;运行&lt;/h3&gt;
&lt;p&gt;通过命令scp放在手机上，用ldid签名然后就可以运行了。&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Jul 2015 00:00:00 +0800</pubDate>
        <link>http://haowai31.github.io//blog/2015/07/14/iossecurity1.html</link>
        <guid isPermaLink="true">http://haowai31.github.io//blog/2015/07/14/iossecurity1.html</guid>
        
        <category>ios安全</category>
        
      </item>
    
      <item>
        <title>usbmuxd服务及应用</title>
        <description>&lt;p&gt;今天在装iPhone上的LLDB+debugserver的时候，wifi突然抽风，非常慢，致使在主机进行lldb远程调试的时候过了好多分钟才能打开远程的app，等待的过程中我查了一下资料，发现也有一些人是遇到了这些情况，他们是通过usbmuxd服务进行解决的。迫不及待，实验了一番。&lt;/p&gt;

&lt;p&gt;后面查到这个usbmuxd服务，将USB通信抽象为TCP通信，也就是说实现了基于USB驱动之上的TCP连接，回想iTunes和Xcode应该也都用了这个技术，从而可以使用USB连接。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/libimobiledevice/libimobiledevice&quot;&gt;libimobiledevice&lt;/a&gt;整合了usbmuxd服务，并在此基础上实现了一些特殊的功能，早期的越狱都是通过这个库进行操作的，同样也可以利用该库+ifuse进行远程访问控制手机等等。（而且是在非越狱的情况下）&lt;/p&gt;

&lt;p&gt;如果说上面这个库还很理论化的话，&lt;a href=&quot;https://github.com/rsms/peertalk&quot;&gt;peerTalk&lt;/a&gt;，依然也是一个基于usbmuxd服务的项目，而且该项目封装的很好，非常容易参考，从而完成自己的开发。而且这中间只是用到的私有的协议，并没有用到私有API，应该还是可以上架的。&lt;/p&gt;
</description>
        <pubDate>Mon, 13 Jul 2015 00:00:00 +0800</pubDate>
        <link>http://haowai31.github.io//blog/2015/07/13/usbmuxd.html</link>
        <guid isPermaLink="true">http://haowai31.github.io//blog/2015/07/13/usbmuxd.html</guid>
        
        <category>ios安全</category>
        
      </item>
    
      <item>
        <title>安全协议读书笔记（二）：安全协议的三大理论分析方法</title>
        <description>&lt;p&gt;安全协议的安全性分析包括理论分析、设计分析、检测分析和经验分析等多种方法。但是理论分析依靠严格的理论验证，使得安全协议可以获得比较高的安全性。
&lt;!-- more --&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;安全多方计算&lt;/h2&gt;

&lt;p&gt;安全多方计算是姚期智在1982年提出的一个概念。之后Glodreich、Micali和Wigderson给出了一般性描述。
目前安全多方计算主要的成果在以下四个方面：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;澄清分布式计算的一些基本安全性问题。&lt;/li&gt;
  &lt;li&gt;说明在既定的安全模型下，哪些分布式计算功能可以安全实现，哪些不能。&lt;/li&gt;
  &lt;li&gt;给出设计分布式安全协议的一般技术和方法。&lt;/li&gt;
  &lt;li&gt;设计可以应用的分布式计算的安全方案和模块。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;到目前位置，理论上任何安全多方计算问题都可以通过普通计算来解决，但是效率比较低。所以设计一个一般性的解决方案很不实用。如何针对特殊情况提出特定的解决方案，如何能使得部署解决方案快速、有效的部署，并能方便的进行二次开发与定制，是现在安全多方计算的一个重要的研究方向。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;安全协议的形式化分析方法&lt;/h2&gt;

&lt;p&gt;这是一种标准的方法，使得所有协议均有可能被证伪，但是参考哥德尔定理，并不能保证能证明，只是可以检查协议符合既定的安全目标。因此形式化协议分析有助于：(1)准确的描述协议的行为；(2)准确的描述出协议的安全特性；(3)证明安全协议满足既定安全目标，以及证明协议在什么条件下不满足既定的安全目标。&lt;/p&gt;

&lt;p&gt;发展过程分为四个阶段。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;早期阶段，这一阶段主要是针对具体的协议进行研究。最早提出形式化分析思想的Needham和Schroeder，为密钥共享和公钥认证服务器系统建立了安全协议。&lt;/li&gt;
  &lt;li&gt;形式化分析初级阶段，以Dolev-Yao的工作为标志。使用BAN类逻辑，CKT5等基于知识逻辑的有效逻辑进行验证。&lt;/li&gt;
  &lt;li&gt;转折阶段。G. Lowe的论文《关于Needham-Schroeder公钥协议的一个攻击》，使得各方开始研究使用一般用途的模型检测方法应用与协议分析。&lt;/li&gt;
  &lt;li&gt;理论证明阶段。Fabrega、Herzog和Guttman的串空间（Strand Space）理论，以及Paulson的归纳方法为代表。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section-2&quot;&gt;安全协议的可证明安全性理论&lt;/h2&gt;

&lt;p&gt;到现在为止，大多数安全协议的现状是，设计出来之后进行一些测试，然后该协议自应用之后很长一段时间都没有被破译，那就具有公认的安全性。如果发现了其中的安全漏洞，在进行少量改动之后，继续进行应用，直到具有公认的安全性。
这个过程，在计算机的各个领域都出现过这种原始而初级的阶段，跟闹着玩儿似的，但是由于完成比完美重要的多，所以先做个能用的是大多数计算机技术人员最常用的选择。
而可证明安全性理论，指的是这么一种归纳的方法：确定安全目标，构建攻击者模型，对某个元操作（Atomic Primitives，比如DES加密算法，某个数学难题等等）的特定协议，然后基于以上形式化模型分析。换句话说对所有的安全协议分析到到最后都会被归纳到对元操作的安全性分析。
可知，可证明安全性理论本质是一种公理化的研究方法。具体我理解也不够深入，这儿给出几个概念，如果感兴趣可以继续查：最初的思想阐述由Goldwasser、Micali和Rivest在20世纪80年代完成，由于效率问题20世纪90年代中出现“面向实际的可证明安全性（Practive-Oriented Provable-Security）”，Bellare和Rogaway提出的随机预言（Random Oracle，RO）模型方法论。RO是一个转折点，之后大量的有效的方案纷纷出现，同时产生了另一个概念：“具体安全性（Contrete Security or Exact Security）”。目前为止，几乎所有的国际安全标准体系都要求至少提供在RO模型中可证明的安全性设计。而现在可证明安全性的方案大都基于RO模型。&lt;/p&gt;

</description>
        <pubDate>Mon, 15 Jun 2015 00:00:00 +0800</pubDate>
        <link>http://haowai31.github.io//blog/2015/06/15/methods_of_protocols_analysis.html</link>
        <guid isPermaLink="true">http://haowai31.github.io//blog/2015/06/15/methods_of_protocols_analysis.html</guid>
        
        <category>安全协议</category>
        
      </item>
    
      <item>
        <title>安全协议读书笔记（一）</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;选书为北京邮电大学出版的《安全协议》，作者是曹天杰、张永平、汪楚娇。这本书是上课时候的教材，由于很多内容囫囵吞枣，在此梳理一遍，更新时间不定。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;section&quot;&gt;攻击模型&lt;/h2&gt;
&lt;p&gt;该攻击模型来源是Dolev和Yao在1983年发的论文。
一般默认攻击者具有以下能力：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;可以窃听所有经过网络的消息；&lt;/li&gt;
  &lt;li&gt;可以阻止和截获所有经过网络的消息；&lt;/li&gt;
  &lt;li&gt;可以存储所获得或自身创造的消息；&lt;/li&gt;
  &lt;li&gt;可以根据存储的消息伪造消息，并发送该消息；&lt;/li&gt;
  &lt;li&gt;可以作为合法的主题参与协议的运行；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;而常见的协议攻击手段包括：窃听、篡改、重放、预重放、反射、拒绝服务、类型攻击、密码分析、证书操纵、协议交互等。
下面分别对反射、类型攻击、证书操纵三种攻击方式详解。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;反射&lt;/h3&gt;

&lt;p&gt;首先，反射是重放的一个特例，该攻击存在的前提是协议能够并行运行。
现在在A和B之间存在一个认证协议，通过这个协议的验证可以使得A和B互相验证，其中\(N_A\)和\(N_B\)分别是A和B生成的一个随机数，K是A和B之间共享的密钥，A和B通过认证对方能揭开自己加密的随机数的方式认证，对方与自己同时拥有密钥K从而进行相互认证。协议过程如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A→B: {\(N_A\)}K&lt;/li&gt;
  &lt;li&gt;B→A: {\(N_B\)}K, \(N_A\)&lt;/li&gt;
  &lt;li&gt;A→B: \(N_B\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A收到消息2的时候就可以认为此消息来源为B，同样B在收到消息3时就可以认为消息来源为A，因为A和B共享密钥K。但是，该协议就可以被反射攻击，攻击过程如下，其中第2,3,6步是攻击者C重新和A发起的另外一个并行认证协议。以下表示协议执行顺序，但是该过程包含了两个并行执行的认证协议。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A→C: {\(N_A\)}K&lt;/li&gt;
  &lt;li&gt;C→A: {\(N_A\)}K&lt;/li&gt;
  &lt;li&gt;A→C: {\(N_A\)’}K, \(N_A\)&lt;/li&gt;
  &lt;li&gt;C→A: {\(N_A\)’}K, \(N_A\)&lt;/li&gt;
  &lt;li&gt;A→C: \(N_A\)’&lt;/li&gt;
  &lt;li&gt;C→A: \(N_A\)’&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;消息1是A发起的一个认证协议，在此，称其为认证协议M，在消息1之后，C收到了用K加密的信息，立即向A发起了一个新的认证协议（认证协议N），发送的就是消息1收到的使用K加密的随机数\(N_A\)。对于认证协议N来说，A需要向C发送一个\(N_A\)的明文，才能向C证明A知道密钥K。同时也会发送一个由K加密的随机数\(N_A\)’。而对于认证协议M来说，C需要向A发送\(N_A\)的明文才能向A证明C保有密钥K，之后C做了两次重放，就完成了认证协议M和N的认证，而在此过程中，所有的解密工作由A完成，C在不保有密钥K的情况下完成了认证。称C完成了一次反射攻击。&lt;/p&gt;

&lt;p&gt;同时在完成攻击之后，C同时获得，获取任意明文被密钥K加密成密文的能力，这在别的攻击中非常有用，比如随时获取明文-密文对，有助于破解密钥K。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;类型攻击&lt;/h3&gt;

&lt;p&gt;由于在协议当中，各方收到的消息都是二进制串组成的，用户没办法将该二进制串区分开，哪部分是加密的，哪部分是明文都不清楚。类型攻击就是利用这一点，让用户将一个消息错误的理解成为另外一个消息，比如可以将身份标识识别为一个密钥。下面是一个例子，该协议是Otway和Rees认证协议。A和B都长期存有与服务器S的密钥\(K_AS\)和\(K_BS\)。A和B的相互认证需要通过S进行，之后S会向A和B发送一个会话密钥\(K_AB\)，具体流程如下，其中M和\(N_A\)是A选择的随机数，\(N_B\)是B的随机数。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A→B: M, A, B, {\(N_A\), M, A, B}\(K_{AS}\)&lt;/li&gt;
  &lt;li&gt;B→S: M, A, B, {\(N_A\), M, A, B}\(K_{AS}\), {\(N_A\), M, A, B}\(K_{BS}\)&lt;/li&gt;
  &lt;li&gt;S→B: M, {\(N_A\), \(K_{AB}\)}\(K_{AS}\), {\(N_A\), M, A, B}\(K_{BS}\)&lt;/li&gt;
  &lt;li&gt;B→A: M, {\(N_A\), \(K_{AB}\)}\(K_{AS}\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;可以看到，消息1和消息4的格式比较相似，这就是进行类型攻击的点。而类型攻击需要额外进行一些假设，就是想要替换的类型长度是一致的，比如这里如果想用M、A、B替换\(K_{AB}\)，那么这两者长度要一致，这里假设一致。在这些假设之后，攻击就可以开始进行了。这里\(C_B\)表示C假冒B进行攻击。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A→\(C_B\): M, A, B, {\(N_A\), M, A, B}\(K_{AS}\)&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
  &lt;li&gt;\(C_B\)→A: M, {\(N_A\), M, A, B}\(K_{AS}\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;可以看到，在消息4中，C直接把从A发送的消息1，部分重新发送给消息A，使得A误认为组合域M、A、B当作共享密钥\(K_{AB}\)，而M、A、B的值都是明文，可以得到，所以在之后的会话中，C就可以继续假冒B与A进行通信了。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;证书操纵&lt;/h3&gt;
&lt;p&gt;数字证书可以担保某实体是公钥的拥有者。但是如果没有验证声明拥有密钥对拥有密钥权限的实体时，就会存在潜在攻击，使得攻击者具有能力获取合法的公钥证书，即使攻击者并不保有对应的私钥。
比如，A和B分别拥有公钥\(g^a\)和\(g^b\)，对应的私钥分别是a和b。A和B分别拥有证书Cert(A)和证书Cert(B)。证书中有公钥的副本。该协议是用来进行密钥协商，这里x和y是A和B选择的随机数。该协议描述如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A→B: \(g^x\), Cert(A)&lt;/li&gt;
  &lt;li&gt;B→A: \(g^y\), Cert(B)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;之后双方计算共享密钥\(K_{AB}=g^{ay+bx}\)。A和B都使用x和y进行计算。攻击者声明自己拥有公钥\(g^{ac}\)，并拥有证书Cert(C)，但是其实攻击者并没有私钥ac。在声明之后，C完成了与A和B的各一次认证，同时进行，攻击过程如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A→\(C_B\): \(g^x\), Cert(A)&lt;/li&gt;
  &lt;li&gt;C→B: \(g^x\), Cert(C)&lt;/li&gt;
  &lt;li&gt;B→C: \(g^y\), Cert(B)&lt;/li&gt;
  &lt;li&gt;\(C_B\)→A: \(g^{yc}\), Cert(B)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A计算密钥\(K_{AB}=(g^{yc})^a*(g^b)^x=g^{acy+bx}\)，B将计算密钥\(K_{AB}=(g^{ac})^y*(g^x)^b=g^{acy+bx}\)。可以看到A和B计算出的密钥是一样的。但是在这里，对于A来说，A认为A和B（其实是C假冒的）保有密钥\(K_AB\)，而对于B来说，B认为B和C保有这个密钥。可以看到存在很多问题。&lt;/p&gt;

</description>
        <pubDate>Thu, 11 Jun 2015 00:00:00 +0800</pubDate>
        <link>http://haowai31.github.io//blog/2015/06/11/Security_Protocols.html</link>
        <guid isPermaLink="true">http://haowai31.github.io//blog/2015/06/11/Security_Protocols.html</guid>
        
        <category>安全协议</category>
        
      </item>
    
      <item>
        <title>NG机器学习的编程实验</title>
        <description>&lt;p&gt;&lt;a href=&quot;#ex1&quot;&gt;实验一：线性回归&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#ex2&quot;&gt;实验二：逻辑回归&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#ex3&quot;&gt;实验三：神经网络&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#ex4&quot;&gt;实验四：神经网络反向传播算法&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#ex5&quot;&gt;实验五：线性回归和修正欠拟合与过拟合&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#ex6&quot;&gt;实验六：支持向量机&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#ex7&quot;&gt;实验七：K-均值算法与PCA算法&lt;/a&gt;&lt;br /&gt;
&lt;!-- more --&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ex1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;programming-assignment-linear-regression&quot;&gt;实验一，Programming Assignment: Linear Regression&lt;/h2&gt;

&lt;p&gt;这个实验就是用来熟悉一下提交的环境，另外在学习算法的各个过程都需要填几行关键程序，包括怎么求估价函数，怎么施行阶梯算法，怎么标准化数据等等，选做实验是两种特征的训练数据。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;1.1 单位矩阵&lt;/h3&gt;
&lt;p&gt;就是通过eye函数得一个单位矩阵。&lt;/p&gt;

&lt;h3 id=&quot;j&quot;&gt;1.2 计算估价函数J&lt;/h3&gt;
&lt;p&gt;（对不起我还不会写公式，回来更新）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;J = sum((X * theta - y).^2) / (2*m);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意其中的.^，这是矩阵特有的操作符号，是矩阵的每一个对应的位置进行相应的运算。sum函数则是对矩阵求和。如果得出的结果是32.07就说明程序写的没问题，总之矩阵运算一定要注意的是x*y对应起来，否则如果出错还容易发现矩阵运算没有写错，如果恰好没有提示错误，得出的结果不对这种Bug会改的头破血流。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;1.3 实现阶梯下降算法&lt;/h3&gt;
&lt;p&gt;（公式仍然不会写）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;theta(1) = theta(1) - alpha/m*sum(X*theta_backup-y);
theta(2) = theta(2) - alpha/m*sum((X*theta_backup-y).*X(:,2));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这只是单纯照着阶梯下降算法更新theta就好。值得注意的地方在上课的时候NG也说了，因为再更新theta的过程中会改变theta，所以我这里用了一个 ** theta_backup ** 对原来的theta进行了备份。这是一个特征的情况，下面还会有多种特征的简便写法（我第一次写的是循环^o^）。&lt;/p&gt;

&lt;p&gt;这个实验做到这里其实已差不多了，看到最后画出的图还是很开心的，虽然感觉上跟我并没有什么关系。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;1.4 多类特征的附加实验&lt;/h3&gt;
&lt;p&gt;具体步骤跟单类特征差不多，值得注意的就是上文提到的更新theta的简便写法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;theta = theta - alpha / m * X' * (X * theta - y); 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;X’是求X的转置矩阵。&lt;/p&gt;

&lt;p&gt;这是对数据标准化的语句，话说在pdf中间直接告诉要怎么做这样真的好嘛？连需要用的函数都给了。。。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mu = mean(X);
sigma = std(X);
X_norm = (X - repmat(mu, size(X,1) , 1))  ./ repmat(sigma,size(X,1),1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;mean()函数是求平均值，具体自己做做实验就知道了，我就是一边儿一边儿在shell上面测试函数的用法，实用是学习最快的方法。std()函数是求标准差，然后完全按照pdf的描述就可以写下这条语句了（当然，我这么笨的人在shell上面测试了好久。。）。&lt;/p&gt;

&lt;p&gt;另外视频中也提到过，如果数据规模小的话，可以直接使用矩阵进行计算：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;theta = pinv(X'*X)*X'*y;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pinv()函数是求矩阵的逆，其实就是这个函数限制着这种干脆利索的方法的运行速度的。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;1.5 总结&lt;/h3&gt;
&lt;p&gt;总之，第一次实验叫做 &lt;strong&gt;worm exercises&lt;/strong&gt; 也是有一定道理的。其实就是给 &lt;strong&gt;纸上得来&lt;/strong&gt; 和 &lt;strong&gt;恭行&lt;/strong&gt; 两件事建立个联系，知道这些算法是可以实现并且实际工作的，这对于我这样的笨蛋还是很重要的，学的时候总觉得隔层纱，到真的程序运行结果砸在脸上了，就能迷糊回来了。&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ex2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;logistic-regression&quot;&gt;实验二，Logistic Regression&lt;/h2&gt;

&lt;p&gt;该实验总共包含两个小实验，一个是基础的逻辑回归实验，另一个是情况稍微复杂一点的实验，需要拟合的数据不是直线，当然其实如果没想那么多，跟着pdf做的话，几乎感觉不到区别，除了最后可视化的图不太一样。&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;2.1 将数据可视化&lt;/h3&gt;

&lt;p&gt;可以看到，边界线大概是条直线。&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;2.2 实现&lt;/h3&gt;

&lt;p&gt;经过前面的实验熏陶，已经知道这个实验实现部分肯定包括CostFunction，下降率，自己对数据进行预测等等，所有需要做的就是照着公式用octave实现出来就可，再次强调 &lt;strong&gt;矩阵的行和列&lt;/strong&gt; 是验算式子写的对不对的有效的方法。
sigmoid函数很简单，不过注意的是这个函数接受的参数z，是矩阵，所以使用操作符号的时候一定要注意，该用“./”就要用。
求J的CostFunction，解析起来比较复杂，还是惯例，从里到外解析，注意运算符左右的矩阵的行和列，一般不会有什么问题，具体公式如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;J = 1 / m * sum((-y).*log(sigmoid(X*theta)) - (1-y).*log(1-sigmoid(X*theta)));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要注意的还是诸如“.*”这些操作符号。
而相关的gradient更新的公式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grad = 1 / m * (X' * (sigmoid(X*theta) - y));
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;
&lt;p&gt;其实costfunction写完之后，需要修改的文件已经差不多没了，但是任务并没有结束，有了costfunction要如何更新theta呢？
这里是使用了octave里面提供的fminunc函数，通过这个函数算出对于CostFunction来说最优的参数。使用方式如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;options = optimset('GradObj', 'on', 'MaxIter', 400);
[theta, cost] = fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GradObj参数告诉fminunc使用gradient来计算需要返回theta和cost两个值，设置MaxIter最大步数为400。后面@(t)的写法是内部调用函数，同时提供theta和上文提到的options就可以等待fminunc算出结果了。并不用自己制定下降率之类的参数，大部分工作都由fminunc做了。&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;2.3 评估结果&lt;/h3&gt;

&lt;p&gt;这里就是将已经得到的theta带进h函数就好，然后把所有的值在0.5以上的数据挑出来，将其y置1就好，使用find，很容易达到目的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmp = sigmoid(X*theta);
finde = find(tmp&amp;gt;=0.5);
p(finde,1) = 1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以看到结果啦。&lt;/p&gt;

&lt;h3 id=&quot;regularized-logistic-regression&quot;&gt;2.4 第二个实验：Regularized logistic regression&lt;/h3&gt;

&lt;p&gt;整个流程和第一个实验差不多，需要注意的是costFunction中计算J函数和gradient更新，对于j=0和j&amp;gt;0是不一样的处理方式，另外在mapFeature函数中，使用了6次的多项式对数据进行拟合，可以看其拟合的手法，值得学习。
最后还给出了过度拟合与拟合度不够的边界值的图，可以看看。&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;2.5 总结&lt;/h3&gt;

&lt;p&gt;整体来说，大部分程序不需要自己动手，只是部分函数自己填一下就可以了，像填空一样，但是做到现在，应该慢慢对整个体系有一些印象，回来是需要自己把整个过程都写出来的，提前做准备。&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ex3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-8&quot;&gt;实验三，神经网络算法&lt;/h2&gt;

&lt;p&gt;这个实验使用了视频最后讲的识别手写数字作为数据输入，分别使用了多种特征的逻辑回归算法与神经网络算法进行实验，用来加强这两种算法的对比。实际运行时也会有个大概的印象，在特征较多时&lt;strong&gt;神经网络算法果然比逻辑回归要快&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;section-9&quot;&gt;3.1 多重特征的逻辑回归算法&lt;/h3&gt;

&lt;p&gt;这一小节就是对上一次实验的复习，如果上次实验costFunction和grad写的好的话，这次可以直接拿来用。但是果然还是再做一边比较好。
然后调用fmincg函数，比之前用的fminunc在处理大规模数据方面更有效率。
但是使用方法和fminunc几乎一样。
然后根据前面训练出来的theta对手写数字进行预测，一切都差不多。需要注意的就是这里使用max函数，得出的是每行最大的值（越大越正确），需要用一个两列的矩阵接受max函数的返回值，这样，第一列返回的是最大值，第二列返回的是最大值所在的位置。第二列才是我们需要的。第几个最大，预测就是几。&lt;/p&gt;

&lt;h3 id=&quot;section-10&quot;&gt;3.2 神经网络算法&lt;/h3&gt;

&lt;p&gt;所有的theta已经算好，而且模型也给出了，是一个具有一个hidden层的模型，hidden层具有25个单元，输出层有10个单元。
那由于数据是有4000个特征的。所以可以预测theta1是25×401的矩阵，而theta2是10×26的矩阵。
于是有了theta之后，就可以一层一层的进行计算了，每层记得加上x0。
如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmpX = [ones(m,1) X];
layer1 = sigmoid(tmpX * Theta1');
tmplayer1 = [ones(m,1) layer1];
layer2 = sigmoid(tmplayer1 * Theta2');
[a,p] = max(layer2,[],2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码很简单，最后的max函数上面也讲过了，需要用一个两列的矩阵接受返回值，第二列就是我们的预测值。（他把10当作0了）&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ex4&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-11&quot;&gt;实验四 神经网络与反向传播算法&lt;/h2&gt;

&lt;p&gt;本次实验主要实现了包括反向传播算法在内的比较完整的神经网络学习算法，由于这个算法比较复杂，实现起来步骤也比较多，所以经常容易出错，甚至这个算法后面还花了很大的篇幅给了一个校验算法，可能都觉得这算法偷摸在后台跑着很不让程序员放心。
本次实验将神经网络学习算法分为了两个部分，第一是前向传播，这部分和上个实验的相关部分相近，神经网络模型也是三层，Theta1是25 × 401，Theta2是10 × 26。在之后写代码的时候会经常遇到。&lt;/p&gt;

&lt;h3 id=&quot;section-12&quot;&gt;4.1 前向算法的实现&lt;/h3&gt;

&lt;p&gt;具体步骤和实验三类似，但是需要注意的是，这里输出层包含了十个单元，但是给出的y只是1-10的数字，需要将y转成位图存储，具体代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Y = [];
E = eye(num_labels);
for i=1:num_labels
	Y0 = find(y==i);
	Y(Y0,:) = repmat(E(i,:),size(Y0,1),1);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要用了find函数与repmat函数，find函数前面有介绍，而repmat函数就是重复矩阵，所以这里又要说了，矩阵运算赛高～～&lt;/p&gt;

&lt;p&gt;这里的costFunction看起来非常复杂，需要算有三个求和符号的式子，而且就算是向量算式仍然还有两个求和符号，善用sum求和函数即可，复杂但是不难。对了，记得加上正则化的参数。&lt;/p&gt;

&lt;p&gt;learning部分，包括梯度导数都将在下一节介绍。&lt;/p&gt;

&lt;h3 id=&quot;section-13&quot;&gt;4.2 后向传播算法的实现&lt;/h3&gt;
&lt;p&gt;这一次实验的重点来了，梯度导数算起来步骤繁多，还是善用解构的方法，现处理和别的联系不大的模块，官方给的pdf也是先要处理sigmoid导数计算的小模块，pdf中已经给出了导数的式子，没事儿可以自己求个导～
将随机初始化处理了之后，就开始本次实验的重头戏，后向传播算法。官方pdf中显然也是非常不放心将步骤这么多的一个算法交给我们这些小朋友，所以步骤都给清楚了，而我在做的时候，也是老老实实重新看了一边视频，将重点重新复习了一下才开始动笔写的。好了严格按照步骤写就好。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;step 1&lt;/strong&gt;&lt;br /&gt;
求a1，我们知道a1就是X中某一个样本特征，但是记得加上bias。由于要算偏差值和delta，所以需要再进行一边前向传播，分别算出a2，a3，z2，z3。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;step 2&lt;/strong&gt;&lt;br /&gt;
算偏差值，err3单独算，然后按照给的公式一步一步算出err2就好。这里给出err2的计算式子。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err_2 = (Theta2' * err_3);
err_2 = err_2(2:end) .* sigmoidGradient(z_2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里切记切记，z2是25 × 1，而err2是26 × 1，需要将bias干掉再算，当然如果在写代码的时候时刻记着现在矩阵计算的行和列，那就不会像我一样犯错误了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;step 3&lt;/strong&gt;&lt;br /&gt;
计算delta1和delta2。按照公式计算即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;delta_2 = delta_2 + (err_3)*a_2';
delta_1 = delta_1 + (err_2)*a_1';
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;step 4&lt;/strong&gt;
计算两个梯度导数，还是一样按照公式计算即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Theta1_grad = 1 / m * delta_1 + lambda / m * theta1_tmp;
Theta2_grad = 1 / m * delta_2 + lambda / m * theta2_tmp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实按照步骤分解，每一步视频都讲的挺清楚，注意代码编写正确即可。&lt;/p&gt;

&lt;p&gt;之后官方已经给了校验算法，自己跑一遍ex4就行。&lt;/p&gt;

&lt;h3 id=&quot;section-14&quot;&gt;4.3 关于隐藏层的可视化&lt;/h3&gt;

&lt;p&gt;这个额外的实验就一个问题：隐藏层究竟代表了什么？
其实咱们小时候学习的时候也是这样，学写字，大概长这个样子就是什么字儿，大概是这个样子是什么句子（还记得那个文字顺序一点儿都不影响阅读么？）。原来在看名家在争论白马非马的时候就跟着思考过这个概念，就是&lt;strong&gt;大概长这个样子&lt;/strong&gt;用不是人话描述就叫模型，所谓马为天下之马而白马就这一匹而已。之所以这个算法叫神经网络算法，其实也是这样，每一个中间层可能就是对某一类事物的模型，每层递进的过程中都在一点儿一点儿完善这个模型。&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ex5&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-15&quot;&gt;实验五 线性回归与修正过拟合与欠拟合&lt;/h2&gt;
&lt;p&gt;这个实验包含了三个实验，一个是通过学习选取比较好的线性回归模型，一个是通过观察学习曲线来修正过拟合与欠拟合，另一个是通过学习选取多项式特征模型，通过学习，选取表现较好的。&lt;/p&gt;

&lt;h3 id=&quot;section-16&quot;&gt;5.1 实现线性拟合&lt;/h3&gt;
&lt;p&gt;就是回顾之前的线性拟合的各种算法。按照pdf上写的CostFunction和梯度选择的公式写就行了。&lt;/p&gt;

&lt;h3 id=&quot;section-17&quot;&gt;5.2 学习曲线&lt;/h3&gt;
&lt;p&gt;分别计算训练组和交叉验证组的误差。为了获得不同的训练组集合，这里只是要求大小不同，所以这里使用了循环来依次改变训练组集合的大小，而交叉验证组单独给出。
这里使用trainLinearReg函数得出theta，然后使用上个实验写好的linearRegCostFunction函数求出各个误差，这里lambda为0。
从得出的训练集的大小与误差的曲线图来看，上个实验得出的线性拟合模型工作的很不错。在下一个实验就会给出使用多项式系数继续进行拟合的方法，通常来说那样做会拟合的更好。&lt;/p&gt;

&lt;h3 id=&quot;section-18&quot;&gt;5.3 使用多项式系数拟合&lt;/h3&gt;
&lt;p&gt;如何表示多项式系数是一个问题，这里先预处理，算出所有可能的x的次方，算出的X_poly是一个m×p的矩阵，这个矩阵每一列都是一个x的次方，从x的1次方到x的p次方。在polyFeatures.m中进行计算就好。很简单。
然后次方数p根据学习选择之后，最后一个问题就是确定lambda。同样如何确定lambda呢？这里还是采用的循环的方式，题目提供了一个lambda可能的集合：{0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10}。可以看到，这就是原来讲过的lambda取值的方式，ng经常这样取，然后根据训练集与交叉验证集的方式选择出比较恰当的lambda。&lt;/p&gt;

&lt;h3 id=&quot;section-19&quot;&gt;5.4 之后的事情&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;事情远没有结束。&lt;/strong&gt;
首先要使用剩下的测试集来验证lambda是否选择适当。而test就是做这个事情的。
而前面选择训练集的时候，并没有做随机化，是依次从前多少个中取得的训练集，这样做会使得学习的结果更加局部化，只是满足局部的训练集，所以这里应该是随机化选择训练集。不过也可以理解，如果是随机化选择训练集，那么结果就没办法评分了。但是这儿还是应该随机化选择的。&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ex6&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-20&quot;&gt;实验六 支持向量机&lt;/h2&gt;
&lt;p&gt;这个实验一共包含了两个实验，一个是支持向量机的实验，包含了三个dataset，第一个是线性的后两个个是非线性的。第二个实验是在垃圾邮件分类方面应用支持向量机进行计算。&lt;/p&gt;

&lt;h3 id=&quot;section-21&quot;&gt;6.1 高斯核函数&lt;/h3&gt;
&lt;p&gt;实现高斯核函数，计算某点和l之间距离的时候可以通过向量进行快速计算。&lt;/p&gt;

&lt;h3 id=&quot;section-22&quot;&gt;6.2 确定两个常数&lt;/h3&gt;
&lt;p&gt;确定C与sigma这两个在高斯核函数中的常数。通过使用支持向量机进行训练，并对交叉验证组进行验证，在一定范围内选出比较合适的C和sigma即可。
首先，C和sigma的范围怎么算，这里提示说仍然使用从0.01依次乘以3进行选取，所以为了程序好写，则需要将可能的C和sigma放在向量中。然后就对不同的C和sigma进行排列组合，训练之后算出交叉验证组的值，选择误差最小的。
Octave里面有svm相关的函数，但是pdf中稍微提了一句，并没有告诉具体的用法，我也是在ex6中找到的，然后查了查资料，然后把这个函数写出来了，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = svmTrain(X, y, C_vec(i), @(x1, x2) gaussianKernel(x1, x2, sigma_vec(j)));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这中间C_vec和sigma_vec两个向量就是C和sigma的取值范围。
算出model之后，需要利用model对交叉验证组进行对照，然后算出误差。这里不再贴代码。
之后选择误差最小的即可。其中可能要用到ind2sub函数，可以自行百度。&lt;/p&gt;

&lt;h3 id=&quot;section-23&quot;&gt;6.3 垃圾邮件分类&lt;/h3&gt;
&lt;p&gt;实验已经对邮件的预处理，可以看看中间都有什么处理，思路还是很重要的，但是这里主要讲实验。
不过其实实验也没啥讲的，在预处理之后，需要完成的就是对单个单词的比对，而且敏感词表也已经给了，如果存在这个敏感词则在一个很长很长的向量中标注一下即可。
然后万事俱备，（纳尼，我啥都没干呢！）就可以使用svm算法对垃圾邮件进行分类了。
之后给的两个附加实验倒是稍微有意思一点，一个就是用现有的model测试自己的邮件，第二个就更彻底一点儿，从SpamAssassin Public Corpus上面下到一些可以作为实验数据的东西，然后自己重新组织程序，通过processEmail和emailFeatures两个函数获得自己的X和y，然后进行训练。记得把实验数据分成训练组、交叉验证组和测试组。&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ex7&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;kpca&quot;&gt;实验七 K均值算法与PCA&lt;/h2&gt;

&lt;p&gt;本次实验肯定分为两部分，一部分K均值，一部分是PCA。而K均值的实验中，一部分是有一个可以可视化的数据，可以追踪聚类中心的移动情况，另一部分就是使用K均值对图像进行压缩。而PCA则是一次普通的实验。&lt;/p&gt;

&lt;h3 id=&quot;k&quot;&gt;7.1 K均值的实现&lt;/h3&gt;
&lt;p&gt;K均值一共需要迭代的步骤是两步，第一步是给每个点找到距离最近的聚类中心，第二步是计算每个聚类的平均值并以此更新聚类中心。
找到最近的聚类中心，无非是把所有的点枚举一遍，然后计算每个点到所有聚类中心的距离，挑出最小的，将每个点的聚类索引更新，这个过程如果用矩阵去写会非常好写。关键步骤如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmp(j) = sum((centroids(j,:)- X(i,:)).^2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中i是枚举点的循环变量，j是枚举聚类中心的循环变量。利用矩阵将距离计算出来。&lt;/p&gt;

&lt;p&gt;第二步流程更为简单，将每个聚类中的所有点算出均值即可。由于已经有了idx，表示每个点所属的聚类中心，那么利用find函数首先找到属于同一聚类中心的点，然后进行均值求解即可。关键步骤如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;centroids(i,:) = mean(X(find(idx==i),:));
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;k-1&quot;&gt;7.2 随机初始化与利用K均值算法压缩图像&lt;/h3&gt;
&lt;p&gt;这一节虽然没有需要提交的代码，但是这儿提供了一个随机初始化的方法，和将图像压缩的方法，可以应用在别的项目中。&lt;/p&gt;

&lt;h3 id=&quot;pca&quot;&gt;7.3 PCA&lt;/h3&gt;
&lt;p&gt;这一节课上老师讲的特别清楚，包括怎么算，为什么这么算，还有在octave中有什么函数方便算。比如求PCA的函数，先选出sigma，然后用svd函数算出U，S，V。其中，U，S在课上讲过用处，U就是矩阵的特征向量，S可以用来选取K。选出矩阵U之后，就可以按照K进行映射了，进行一次矩阵乘法即可。恢复数据是一样的，这个实验主要还是用来熟悉流程，看到一个一个例子在机器上运行，感觉很踏实就好。&lt;/p&gt;

&lt;h3 id=&quot;pca-1&quot;&gt;7.4 PCA应用&lt;/h3&gt;
&lt;p&gt;之后给了两个应用PCA的例子，一个是脸部的特征的降维，从1024维降到100维。第二个实验是利用PCA进行可视化，将3维的降到2维的。有兴趣可以看看代码，因为这个代码的基础部件还是咱们自己写的。&lt;/p&gt;

&lt;h2 id=&quot;section-24&quot;&gt;更新历史&lt;/h2&gt;
&lt;p&gt;实验一更新（2015.05.14 21：28）&lt;br /&gt;
实验二更新（2015.05.20 09：50）&lt;br /&gt;
实验三更新（2015.05.27 09：23）&lt;br /&gt;
实验四更新（2015.06.09 09：09）&lt;br /&gt;
实验五更新（2015.06.29 09：17）&lt;br /&gt;
实验六更新（2015.07.11 15：17）&lt;br /&gt;
实验七更新（2015.07.12 12：52）&lt;/p&gt;

</description>
        <pubDate>Thu, 14 May 2015 00:00:00 +0800</pubDate>
        <link>http://haowai31.github.io//blog/2015/05/14/ng_ex.html</link>
        <guid isPermaLink="true">http://haowai31.github.io//blog/2015/05/14/ng_ex.html</guid>
        
        <category>机器学习</category>
        
      </item>
    
      <item>
        <title>你好，旅行者～</title>
        <description>&lt;p&gt;原本打算把之前进行的论文阅读报告记录一下，然而由于&lt;strong&gt;不可抗力&lt;/strong&gt;的原因，最终在github上搭了个博客来完成原来的目标。总之，尽量保持更新。&lt;/p&gt;
</description>
        <pubDate>Tue, 12 May 2015 00:00:00 +0800</pubDate>
        <link>http://haowai31.github.io//blog/2015/05/12/hello.html</link>
        <guid isPermaLink="true">http://haowai31.github.io//blog/2015/05/12/hello.html</guid>
        
        <category>开始</category>
        
      </item>
    
  </channel>
</rss>
