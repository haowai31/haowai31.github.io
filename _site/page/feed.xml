<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RSS - Thinking and Recording</title>
    <description>Thinking and Recording - </description>
    <link>http://haowai31.github.io/</link>
    <atom:link href="http://haowai31.github.io//page/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 29 Jun 2015 11:08:30 +0800</pubDate>
    <lastBuildDate>Mon, 29 Jun 2015 11:08:30 +0800</lastBuildDate>
    <generator>haowai31</generator>
    
      <item>
        <title>安全协议读书笔记（二）：安全协议的三大理论分析方法</title>
        <description>&lt;p&gt;安全协议的安全性分析包括理论分析、设计分析、检测分析和经验分析等多种方法。但是理论分析依靠严格的理论验证，使得安全协议可以获得比较高的安全性。
&lt;!-- more --&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;安全多方计算&lt;/h2&gt;

&lt;p&gt;安全多方计算是姚期智在1982年提出的一个概念。之后Glodreich、Micali和Wigderson给出了一般性描述。
目前安全多方计算主要的成果在以下四个方面：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;澄清分布式计算的一些基本安全性问题。&lt;/li&gt;
  &lt;li&gt;说明在既定的安全模型下，哪些分布式计算功能可以安全实现，哪些不能。&lt;/li&gt;
  &lt;li&gt;给出设计分布式安全协议的一般技术和方法。&lt;/li&gt;
  &lt;li&gt;设计可以应用的分布式计算的安全方案和模块。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;到目前位置，理论上任何安全多方计算问题都可以通过普通计算来解决，但是效率比较低。所以设计一个一般性的解决方案很不实用。如何针对特殊情况提出特定的解决方案，如何能使得部署解决方案快速、有效的部署，并能方便的进行二次开发与定制，是现在安全多方计算的一个重要的研究方向。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;安全协议的形式化分析方法&lt;/h2&gt;

&lt;p&gt;这是一种标准的方法，使得所有协议均有可能被证伪，但是参考哥德尔定理，并不能保证能证明，只是可以检查协议符合既定的安全目标。因此形式化协议分析有助于：(1)准确的描述协议的行为；(2)准确的描述出协议的安全特性；(3)证明安全协议满足既定安全目标，以及证明协议在什么条件下不满足既定的安全目标。&lt;/p&gt;

&lt;p&gt;发展过程分为四个阶段。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;早期阶段，这一阶段主要是针对具体的协议进行研究。最早提出形式化分析思想的Needham和Schroeder，为密钥共享和公钥认证服务器系统建立了安全协议。&lt;/li&gt;
  &lt;li&gt;形式化分析初级阶段，以Dolev-Yao的工作为标志。使用BAN类逻辑，CKT5等基于知识逻辑的有效逻辑进行验证。&lt;/li&gt;
  &lt;li&gt;转折阶段。G. Lowe的论文《关于Needham-Schroeder公钥协议的一个攻击》，使得各方开始研究使用一般用途的模型检测方法应用与协议分析。&lt;/li&gt;
  &lt;li&gt;理论证明阶段。Fabrega、Herzog和Guttman的串空间（Strand Space）理论，以及Paulson的归纳方法为代表。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section-2&quot;&gt;安全协议的可证明安全性理论&lt;/h2&gt;

&lt;p&gt;到现在为止，大多数安全协议的现状是，设计出来之后进行一些测试，然后该协议自应用之后很长一段时间都没有被破译，那就具有公认的安全性。如果发现了其中的安全漏洞，在进行少量改动之后，继续进行应用，直到具有公认的安全性。
这个过程，在计算机的各个领域都出现过这种原始而初级的阶段，跟闹着玩儿似的，但是由于完成比完美重要的多，所以先做个能用的是大多数计算机技术人员最常用的选择。
而可证明安全性理论，指的是这么一种归纳的方法：确定安全目标，构建攻击者模型，对某个元操作（Atomic Primitives，比如DES加密算法，某个数学难题等等）的特定协议，然后基于以上形式化模型分析。换句话说对所有的安全协议分析到到最后都会被归纳到对元操作的安全性分析。
可知，可证明安全性理论本质是一种公理化的研究方法。具体我理解也不够深入，这儿给出几个概念，如果感兴趣可以继续查：最初的思想阐述由Goldwasser、Micali和Rivest在20世纪80年代完成，由于效率问题20世纪90年代中出现“面向实际的可证明安全性（Practive-Oriented Provable-Security）”，Bellare和Rogaway提出的随机预言（Random Oracle，RO）模型方法论。RO是一个转折点，之后大量的有效的方案纷纷出现，同时产生了另一个概念：“具体安全性（Contrete Security or Exact Security）”。目前为止，几乎所有的国际安全标准体系都要求至少提供在RO模型中可证明的安全性设计。而现在可证明安全性的方案大都基于RO模型。&lt;/p&gt;

</description>
        <pubDate>Mon, 15 Jun 2015 00:00:00 +0800</pubDate>
        <link>http://haowai31.github.io//blog/2015/06/15/methods_of_protocols_analysis.html</link>
        <guid isPermaLink="true">http://haowai31.github.io//blog/2015/06/15/methods_of_protocols_analysis.html</guid>
        
        <category>安全协议</category>
        
      </item>
    
      <item>
        <title>安全协议读书笔记（一）</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;选书为北京邮电大学出版的《安全协议》，作者是曹天杰、张永平、汪楚娇。这本书是上课时候的教材，由于很多内容囫囵吞枣，在此梳理一遍，更新时间不定。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;section&quot;&gt;攻击模型&lt;/h2&gt;
&lt;p&gt;该攻击模型来源是Dolev和Yao在1983年发的论文。
一般默认攻击者具有以下能力：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;可以窃听所有经过网络的消息；&lt;/li&gt;
  &lt;li&gt;可以阻止和截获所有经过网络的消息；&lt;/li&gt;
  &lt;li&gt;可以存储所获得或自身创造的消息；&lt;/li&gt;
  &lt;li&gt;可以根据存储的消息伪造消息，并发送该消息；&lt;/li&gt;
  &lt;li&gt;可以作为合法的主题参与协议的运行；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;而常见的协议攻击手段包括：窃听、篡改、重放、预重放、反射、拒绝服务、类型攻击、密码分析、证书操纵、协议交互等。
下面分别对反射、类型攻击、证书操纵三种攻击方式详解。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;反射&lt;/h3&gt;

&lt;p&gt;首先，反射是重放的一个特例，该攻击存在的前提是协议能够并行运行。
现在在A和B之间存在一个认证协议，通过这个协议的验证可以使得A和B互相验证，其中\(N_A\)和\(N_B\)分别是A和B生成的一个随机数，K是A和B之间共享的密钥，A和B通过认证对方能揭开自己加密的随机数的方式认证，对方与自己同时拥有密钥K从而进行相互认证。协议过程如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A→B: {\(N_A\)}K&lt;/li&gt;
  &lt;li&gt;B→A: {\(N_B\)}K, \(N_A\)&lt;/li&gt;
  &lt;li&gt;A→B: \(N_B\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A收到消息2的时候就可以认为此消息来源为B，同样B在收到消息3时就可以认为消息来源为A，因为A和B共享密钥K。但是，该协议就可以被反射攻击，攻击过程如下，其中第2,3,6步是攻击者C重新和A发起的另外一个并行认证协议。以下表示协议执行顺序，但是该过程包含了两个并行执行的认证协议。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A→C: {\(N_A\)}K&lt;/li&gt;
  &lt;li&gt;C→A: {\(N_A\)}K&lt;/li&gt;
  &lt;li&gt;A→C: {\(N_A\)’}K, \(N_A\)&lt;/li&gt;
  &lt;li&gt;C→A: {\(N_A\)’}K, \(N_A\)&lt;/li&gt;
  &lt;li&gt;A→C: \(N_A\)’&lt;/li&gt;
  &lt;li&gt;C→A: \(N_A\)’&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;消息1是A发起的一个认证协议，在此，称其为认证协议M，在消息1之后，C收到了用K加密的信息，立即向A发起了一个新的认证协议（认证协议N），发送的就是消息1收到的使用K加密的随机数\(N_A\)。对于认证协议N来说，A需要向C发送一个\(N_A\)的明文，才能向C证明A知道密钥K。同时也会发送一个由K加密的随机数\(N_A\)’。而对于认证协议M来说，C需要向A发送\(N_A\)的明文才能向A证明C保有密钥K，之后C做了两次重放，就完成了认证协议M和N的认证，而在此过程中，所有的解密工作由A完成，C在不保有密钥K的情况下完成了认证。称C完成了一次反射攻击。&lt;/p&gt;

&lt;p&gt;同时在完成攻击之后，C同时获得，获取任意明文被密钥K加密成密文的能力，这在别的攻击中非常有用，比如随时获取明文-密文对，有助于破解密钥K。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;类型攻击&lt;/h3&gt;

&lt;p&gt;由于在协议当中，各方收到的消息都是二进制串组成的，用户没办法将该二进制串区分开，哪部分是加密的，哪部分是明文都不清楚。类型攻击就是利用这一点，让用户将一个消息错误的理解成为另外一个消息，比如可以将身份标识识别为一个密钥。下面是一个例子，该协议是Otway和Rees认证协议。A和B都长期存有与服务器S的密钥\(K_AS\)和\(K_BS\)。A和B的相互认证需要通过S进行，之后S会向A和B发送一个会话密钥\(K_AB\)，具体流程如下，其中M和\(N_A\)是A选择的随机数，\(N_B\)是B的随机数。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A→B: M, A, B, {\(N_A\), M, A, B}\(K_{AS}\)&lt;/li&gt;
  &lt;li&gt;B→S: M, A, B, {\(N_A\), M, A, B}\(K_{AS}\), {\(N_A\), M, A, B}\(K_{BS}\)&lt;/li&gt;
  &lt;li&gt;S→B: M, {\(N_A\), \(K_{AB}\)}\(K_{AS}\), {\(N_A\), M, A, B}\(K_{BS}\)&lt;/li&gt;
  &lt;li&gt;B→A: M, {\(N_A\), \(K_{AB}\)}\(K_{AS}\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;可以看到，消息1和消息4的格式比较相似，这就是进行类型攻击的点。而类型攻击需要额外进行一些假设，就是想要替换的类型长度是一致的，比如这里如果想用M、A、B替换\(K_{AB}\)，那么这两者长度要一致，这里假设一致。在这些假设之后，攻击就可以开始进行了。这里\(C_B\)表示C假冒B进行攻击。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A→\(C_B\): M, A, B, {\(N_A\), M, A, B}\(K_{AS}\)&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
  &lt;li&gt;\(C_B\)→A: M, {\(N_A\), M, A, B}\(K_{AS}\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;可以看到，在消息4中，C直接把从A发送的消息1，部分重新发送给消息A，使得A误认为组合域M、A、B当作共享密钥\(K_{AB}\)，而M、A、B的值都是明文，可以得到，所以在之后的会话中，C就可以继续假冒B与A进行通信了。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;证书操纵&lt;/h3&gt;
&lt;p&gt;数字证书可以担保某实体是公钥的拥有者。但是如果没有验证声明拥有密钥对拥有密钥权限的实体时，就会存在潜在攻击，使得攻击者具有能力获取合法的公钥证书，即使攻击者并不保有对应的私钥。
比如，A和B分别拥有公钥\(g^a\)和\(g^b\)，对应的私钥分别是a和b。A和B分别拥有证书Cert(A)和证书Cert(B)。证书中有公钥的副本。该协议是用来进行密钥协商，这里x和y是A和B选择的随机数。该协议描述如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A→B: \(g^x\), Cert(A)&lt;/li&gt;
  &lt;li&gt;B→A: \(g^y\), Cert(B)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;之后双方计算共享密钥\(K_{AB}=g^{ay+bx}\)。A和B都使用x和y进行计算。攻击者声明自己拥有公钥\(g^{ac}\)，并拥有证书Cert(C)，但是其实攻击者并没有私钥ac。在声明之后，C完成了与A和B的各一次认证，同时进行，攻击过程如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A→\(C_B\): \(g^x\), Cert(A)&lt;/li&gt;
  &lt;li&gt;C→B: \(g^x\), Cert(C)&lt;/li&gt;
  &lt;li&gt;B→C: \(g^y\), Cert(B)&lt;/li&gt;
  &lt;li&gt;\(C_B\)→A: \(g^{yc}\), Cert(B)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A计算密钥\(K_{AB}=(g^{yc})^a*(g^b)^x=g^{acy+bx}\)，B将计算密钥\(K_{AB}=(g^{ac})^y*(g^x)^b=g^{acy+bx}\)。可以看到A和B计算出的密钥是一样的。但是在这里，对于A来说，A认为A和B（其实是C假冒的）保有密钥\(K_AB\)，而对于B来说，B认为B和C保有这个密钥。可以看到存在很多问题。&lt;/p&gt;

</description>
        <pubDate>Thu, 11 Jun 2015 00:00:00 +0800</pubDate>
        <link>http://haowai31.github.io//blog/2015/06/11/Security_Protocols.html</link>
        <guid isPermaLink="true">http://haowai31.github.io//blog/2015/06/11/Security_Protocols.html</guid>
        
        <category>安全协议</category>
        
      </item>
    
      <item>
        <title>NG机器学习的编程实验</title>
        <description>&lt;p&gt;&lt;a href=&quot;#ex1&quot;&gt;实验一：线性回归&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#ex2&quot;&gt;实验二：逻辑回归&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#ex3&quot;&gt;实验三：神经网络&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#ex4&quot;&gt;实验四：神经网络反向传播算法&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#ex5&quot;&gt;实验五：线性回归和修正欠拟合与过拟合&lt;/a&gt;&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;&lt;a name=&quot;ex1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;programming-assignment-linear-regression&quot;&gt;实验一，Programming Assignment: Linear Regression&lt;/h2&gt;

&lt;p&gt;这个实验就是用来熟悉一下提交的环境，另外在学习算法的各个过程都需要填几行关键程序，包括怎么求估价函数，怎么施行阶梯算法，怎么标准化数据等等，选做实验是两种特征的训练数据。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;1.1 单位矩阵&lt;/h3&gt;
&lt;p&gt;就是通过eye函数得一个单位矩阵。&lt;/p&gt;

&lt;h3 id=&quot;j&quot;&gt;1.2 计算估价函数J&lt;/h3&gt;
&lt;p&gt;（对不起我还不会写公式，回来更新）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;J = sum((X * theta - y).^2) / (2*m);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意其中的.^，这是矩阵特有的操作符号，是矩阵的每一个对应的位置进行相应的运算。sum函数则是对矩阵求和。如果得出的结果是32.07就说明程序写的没问题，总之矩阵运算一定要注意的是x*y对应起来，否则如果出错还容易发现矩阵运算没有写错，如果恰好没有提示错误，得出的结果不对这种Bug会改的头破血流。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;1.3 实现阶梯下降算法&lt;/h3&gt;
&lt;p&gt;（公式仍然不会写）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;theta(1) = theta(1) - alpha/m*sum(X*theta_backup-y);
theta(2) = theta(2) - alpha/m*sum((X*theta_backup-y).*X(:,2));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这只是单纯照着阶梯下降算法更新theta就好。值得注意的地方在上课的时候NG也说了，因为再更新theta的过程中会改变theta，所以我这里用了一个 ** theta_backup ** 对原来的theta进行了备份。这是一个特征的情况，下面还会有多种特征的简便写法（我第一次写的是循环^o^）。&lt;/p&gt;

&lt;p&gt;这个实验做到这里其实已差不多了，看到最后画出的图还是很开心的，虽然感觉上跟我并没有什么关系。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;1.4 多类特征的附加实验&lt;/h3&gt;
&lt;p&gt;具体步骤跟单类特征差不多，值得注意的就是上文提到的更新theta的简便写法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;theta = theta - alpha / m * X' * (X * theta - y); 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;X’是求X的转置矩阵。&lt;/p&gt;

&lt;p&gt;这是对数据标准化的语句，话说在pdf中间直接告诉要怎么做这样真的好嘛？连需要用的函数都给了。。。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mu = mean(X);
sigma = std(X);
X_norm = (X - repmat(mu, size(X,1) , 1))  ./ repmat(sigma,size(X,1),1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;mean()函数是求平均值，具体自己做做实验就知道了，我就是一边儿一边儿在shell上面测试函数的用法，实用是学习最快的方法。std()函数是求标准差，然后完全按照pdf的描述就可以写下这条语句了（当然，我这么笨的人在shell上面测试了好久。。）。&lt;/p&gt;

&lt;p&gt;另外视频中也提到过，如果数据规模小的话，可以直接使用矩阵进行计算：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;theta = pinv(X'*X)*X'*y;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pinv()函数是求矩阵的逆，其实就是这个函数限制着这种干脆利索的方法的运行速度的。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;1.5 总结&lt;/h3&gt;
&lt;p&gt;总之，第一次实验叫做 &lt;strong&gt;worm exercises&lt;/strong&gt; 也是有一定道理的。其实就是给 &lt;strong&gt;纸上得来&lt;/strong&gt; 和 &lt;strong&gt;恭行&lt;/strong&gt; 两件事建立个联系，知道这些算法是可以实现并且实际工作的，这对于我这样的笨蛋还是很重要的，学的时候总觉得隔层纱，到真的程序运行结果砸在脸上了，就能迷糊回来了。&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ex2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;logistic-regression&quot;&gt;实验二，Logistic Regression&lt;/h2&gt;

&lt;p&gt;该实验总共包含两个小实验，一个是基础的逻辑回归实验，另一个是情况稍微复杂一点的实验，需要拟合的数据不是直线，当然其实如果没想那么多，跟着pdf做的话，几乎感觉不到区别，除了最后可视化的图不太一样。&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;2.1 将数据可视化&lt;/h3&gt;

&lt;p&gt;可以看到，边界线大概是条直线。&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;2.2 实现&lt;/h3&gt;

&lt;p&gt;经过前面的实验熏陶，已经知道这个实验实现部分肯定包括CostFunction，下降率，自己对数据进行预测等等，所有需要做的就是照着公式用octave实现出来就可，再次强调 &lt;strong&gt;矩阵的行和列&lt;/strong&gt; 是验算式子写的对不对的有效的方法。
sigmoid函数很简单，不过注意的是这个函数接受的参数z，是矩阵，所以使用操作符号的时候一定要注意，该用“./”就要用。
求J的CostFunction，解析起来比较复杂，还是惯例，从里到外解析，注意运算符左右的矩阵的行和列，一般不会有什么问题，具体公式如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;J = 1 / m * sum((-y).*log(sigmoid(X*theta)) - (1-y).*log(1-sigmoid(X*theta)));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要注意的还是诸如“.*”这些操作符号。
而相关的gradient更新的公式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grad = 1 / m * (X' * (sigmoid(X*theta) - y));
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;
&lt;p&gt;其实costfunction写完之后，需要修改的文件已经差不多没了，但是任务并没有结束，有了costfunction要如何更新theta呢？
这里是使用了octave里面提供的fminunc函数，通过这个函数算出对于CostFunction来说最优的参数。使用方式如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;options = optimset('GradObj', 'on', 'MaxIter', 400);
[theta, cost] = fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GradObj参数告诉fminunc使用gradient来计算需要返回theta和cost两个值，设置MaxIter最大步数为400。后面@(t)的写法是内部调用函数，同时提供theta和上文提到的options就可以等待fminunc算出结果了。并不用自己制定下降率之类的参数，大部分工作都由fminunc做了。&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;2.3 评估结果&lt;/h3&gt;

&lt;p&gt;这里就是将已经得到的theta带进h函数就好，然后把所有的值在0.5以上的数据挑出来，将其y置1就好，使用find，很容易达到目的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmp = sigmoid(X*theta);
finde = find(tmp&amp;gt;=0.5);
p(finde,1) = 1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以看到结果啦。&lt;/p&gt;

&lt;h3 id=&quot;regularized-logistic-regression&quot;&gt;2.4 第二个实验：Regularized logistic regression&lt;/h3&gt;

&lt;p&gt;整个流程和第一个实验差不多，需要注意的是costFunction中计算J函数和gradient更新，对于j=0和j&amp;gt;0是不一样的处理方式，另外在mapFeature函数中，使用了6次的多项式对数据进行拟合，可以看其拟合的手法，值得学习。
最后还给出了过度拟合与拟合度不够的边界值的图，可以看看。&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;2.5 总结&lt;/h3&gt;

&lt;p&gt;整体来说，大部分程序不需要自己动手，只是部分函数自己填一下就可以了，像填空一样，但是做到现在，应该慢慢对整个体系有一些印象，回来是需要自己把整个过程都写出来的，提前做准备。&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ex3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-8&quot;&gt;实验三，神经网络算法&lt;/h2&gt;

&lt;p&gt;这个实验使用了视频最后讲的识别手写数字作为数据输入，分别使用了多种特征的逻辑回归算法与神经网络算法进行实验，用来加强这两种算法的对比。实际运行时也会有个大概的印象，在特征较多时&lt;strong&gt;神经网络算法果然比逻辑回归要快&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;section-9&quot;&gt;3.1 多重特征的逻辑回归算法&lt;/h3&gt;

&lt;p&gt;这一小节就是对上一次实验的复习，如果上次实验costFunction和grad写的好的话，这次可以直接拿来用。但是果然还是再做一边比较好。
然后调用fmincg函数，比之前用的fminunc在处理大规模数据方面更有效率。
但是使用方法和fminunc几乎一样。
然后根据前面训练出来的theta对手写数字进行预测，一切都差不多。需要注意的就是这里使用max函数，得出的是每行最大的值（越大越正确），需要用一个两列的矩阵接受max函数的返回值，这样，第一列返回的是最大值，第二列返回的是最大值所在的位置。第二列才是我们需要的。第几个最大，预测就是几。&lt;/p&gt;

&lt;h3 id=&quot;section-10&quot;&gt;3.2 神经网络算法&lt;/h3&gt;

&lt;p&gt;所有的theta已经算好，而且模型也给出了，是一个具有一个hidden层的模型，hidden层具有25个单元，输出层有10个单元。
那由于数据是有4000个特征的。所以可以预测theta1是25×401的矩阵，而theta2是10×26的矩阵。
于是有了theta之后，就可以一层一层的进行计算了，每层记得加上x0。
如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmpX = [ones(m,1) X];
layer1 = sigmoid(tmpX * Theta1');
tmplayer1 = [ones(m,1) layer1];
layer2 = sigmoid(tmplayer1 * Theta2');
[a,p] = max(layer2,[],2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码很简单，最后的max函数上面也讲过了，需要用一个两列的矩阵接受返回值，第二列就是我们的预测值。（他把10当作0了）&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ex4&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-11&quot;&gt;实验四 神经网络与反向传播算法&lt;/h2&gt;

&lt;p&gt;本次实验主要实现了包括反向传播算法在内的比较完整的神经网络学习算法，由于这个算法比较复杂，实现起来步骤也比较多，所以经常容易出错，甚至这个算法后面还花了很大的篇幅给了一个校验算法，可能都觉得这算法偷摸在后台跑着很不让程序员放心。
本次实验将神经网络学习算法分为了两个部分，第一是前向传播，这部分和上个实验的相关部分相近，神经网络模型也是三层，Theta1是25 × 401，Theta2是10 × 26。在之后写代码的时候会经常遇到。&lt;/p&gt;

&lt;h3 id=&quot;section-12&quot;&gt;4.1 前向算法的实现&lt;/h3&gt;

&lt;p&gt;具体步骤和实验三类似，但是需要注意的是，这里输出层包含了十个单元，但是给出的y只是1-10的数字，需要将y转成位图存储，具体代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Y = [];
E = eye(num_labels);
for i=1:num_labels
	Y0 = find(y==i);
	Y(Y0,:) = repmat(E(i,:),size(Y0,1),1);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要用了find函数与repmat函数，find函数前面有介绍，而repmat函数就是重复矩阵，所以这里又要说了，矩阵运算赛高～～&lt;/p&gt;

&lt;p&gt;这里的costFunction看起来非常复杂，需要算有三个求和符号的式子，而且就算是向量算式仍然还有两个求和符号，善用sum求和函数即可，复杂但是不难。对了，记得加上正则化的参数。&lt;/p&gt;

&lt;p&gt;learning部分，包括梯度导数都将在下一节介绍。&lt;/p&gt;

&lt;h3 id=&quot;section-13&quot;&gt;4.2 后向传播算法的实现&lt;/h3&gt;
&lt;p&gt;这一次实验的重点来了，梯度导数算起来步骤繁多，还是善用解构的方法，现处理和别的联系不大的模块，官方给的pdf也是先要处理sigmoid导数计算的小模块，pdf中已经给出了导数的式子，没事儿可以自己求个导～
将随机初始化处理了之后，就开始本次实验的重头戏，后向传播算法。官方pdf中显然也是非常不放心将步骤这么多的一个算法交给我们这些小朋友，所以步骤都给清楚了，而我在做的时候，也是老老实实重新看了一边视频，将重点重新复习了一下才开始动笔写的。好了严格按照步骤写就好。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;step 1&lt;/strong&gt;&lt;br /&gt;
求a1，我们知道a1就是X中某一个样本特征，但是记得加上bias。由于要算偏差值和delta，所以需要再进行一边前向传播，分别算出a2，a3，z2，z3。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;step 2&lt;/strong&gt;&lt;br /&gt;
算偏差值，err3单独算，然后按照给的公式一步一步算出err2就好。这里给出err2的计算式子。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err_2 = (Theta2' * err_3);
err_2 = err_2(2:end) .* sigmoidGradient(z_2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里切记切记，z2是25 × 1，而err2是26 × 1，需要将bias干掉再算，当然如果在写代码的时候时刻记着现在矩阵计算的行和列，那就不会像我一样犯错误了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;step 3&lt;/strong&gt;&lt;br /&gt;
计算delta1和delta2。按照公式计算即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;delta_2 = delta_2 + (err_3)*a_2';
delta_1 = delta_1 + (err_2)*a_1';
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;step 4&lt;/strong&gt;
计算两个梯度导数，还是一样按照公式计算即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Theta1_grad = 1 / m * delta_1 + lambda / m * theta1_tmp;
Theta2_grad = 1 / m * delta_2 + lambda / m * theta2_tmp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实按照步骤分解，每一步视频都讲的挺清楚，注意代码编写正确即可。&lt;/p&gt;

&lt;p&gt;之后官方已经给了校验算法，自己跑一遍ex4就行。&lt;/p&gt;

&lt;h3 id=&quot;section-14&quot;&gt;4.3 关于隐藏层的可视化&lt;/h3&gt;

&lt;p&gt;这个额外的实验就一个问题：隐藏层究竟代表了什么？
其实咱们小时候学习的时候也是这样，学写字，大概长这个样子就是什么字儿，大概是这个样子是什么句子（还记得那个文字顺序一点儿都不影响阅读么？）。原来在看名家在争论白马非马的时候就跟着思考过这个概念，就是&lt;strong&gt;大概长这个样子&lt;/strong&gt;用不是人话描述就叫模型，所谓马为天下之马而白马就这一匹而已。之所以这个算法叫神经网络算法，其实也是这样，每一个中间层可能就是对某一类事物的模型，每层递进的过程中都在一点儿一点儿完善这个模型。&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ex5&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-15&quot;&gt;实验五 线性回归与修正过拟合与欠拟合&lt;/h2&gt;
&lt;p&gt;这个实验包含了三个实验，一个是通过学习选取比较好的线性回归模型，一个是通过观察学习曲线来修正过拟合与欠拟合，另一个是通过学习选取多项式特征模型，通过学习，选取表现较好的。&lt;/p&gt;

&lt;h3 id=&quot;section-16&quot;&gt;5.1 实现线性拟合&lt;/h3&gt;
&lt;p&gt;就是回顾之前的线性拟合的各种算法。按照pdf上写的CostFunction和梯度选择的公式写就行了。&lt;/p&gt;

&lt;h3 id=&quot;section-17&quot;&gt;5.2 学习曲线&lt;/h3&gt;
&lt;p&gt;分别计算训练组和交叉验证组的误差。为了获得不同的训练组集合，这里只是要求大小不同，所以这里使用了循环来依次改变训练组集合的大小，而交叉验证组单独给出。
这里使用trainLinearReg函数得出theta，然后使用上个实验写好的linearRegCostFunction函数求出各个误差，这里lambda为0。
从得出的训练集的大小与误差的曲线图来看，上个实验得出的线性拟合模型工作的很不错。在下一个实验就会给出使用多项式系数继续进行拟合的方法，通常来说那样做会拟合的更好。&lt;/p&gt;

&lt;h3 id=&quot;section-18&quot;&gt;5.3 使用多项式系数拟合&lt;/h3&gt;
&lt;p&gt;如何表示多项式系数是一个问题，这里先预处理，算出所有可能的x的次方，算出的X_poly是一个m×p的矩阵，这个矩阵每一列都是一个x的次方，从x的1次方到x的p次方。在polyFeatures.m中进行计算就好。很简单。
然后次方数p根据学习选择之后，最后一个问题就是确定lambda。同样如何确定lambda呢？这里还是采用的循环的方式，题目提供了一个lambda可能的集合：{0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10}。可以看到，这就是原来讲过的lambda取值的方式，ng经常这样取，然后根据训练集与交叉验证集的方式选择出比较恰当的lambda。&lt;/p&gt;

&lt;h3 id=&quot;section-19&quot;&gt;5.4 之后的事情&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;事情远没有结束。&lt;/strong&gt;
首先要使用剩下的测试集来验证lambda是否选择适当。而test就是做这个事情的。
而前面选择训练集的时候，并没有做随机化，是依次从前多少个中取得的训练集，这样做会使得学习的结果更加局部化，只是满足局部的训练集，所以这里应该是随机化选择训练集。不过也可以理解，如果是随机化选择训练集，那么结果就没办法评分了。但是这儿还是应该随机化选择的。&lt;/p&gt;

&lt;p&gt;实验一更新（2015.05.14 21：28）&lt;br /&gt;
实验二更新（2015.05.20 09：50）&lt;br /&gt;
实验三更新（2015.05.27 09：23）&lt;br /&gt;
实验四更新（2015.06.09 09：09）&lt;br /&gt;
实验五更新（2015.06.29 09：17）&lt;/p&gt;
</description>
        <pubDate>Thu, 14 May 2015 00:00:00 +0800</pubDate>
        <link>http://haowai31.github.io//blog/2015/05/14/ng_ex.html</link>
        <guid isPermaLink="true">http://haowai31.github.io//blog/2015/05/14/ng_ex.html</guid>
        
        <category>机器学习</category>
        
      </item>
    
      <item>
        <title>你好，旅行者～</title>
        <description>&lt;p&gt;原本打算把之前进行的论文阅读报告记录一下，然而由于&lt;strong&gt;不可抗力&lt;/strong&gt;的原因，最终在github上搭了个博客来完成原来的目标。总之，尽量保持更新。&lt;/p&gt;
</description>
        <pubDate>Tue, 12 May 2015 00:00:00 +0800</pubDate>
        <link>http://haowai31.github.io//blog/2015/05/12/hello.html</link>
        <guid isPermaLink="true">http://haowai31.github.io//blog/2015/05/12/hello.html</guid>
        
        <category>开始</category>
        
      </item>
    
  </channel>
</rss>
