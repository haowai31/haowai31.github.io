---
layout: post
title: NG机器学习的编程实验
category: 机器学习
date: 2015-05-14
---

[实验一：线性回归](#ex1)  
[实验二：逻辑回归](#ex2)  
[实验三：神经网络](#ex3)  
[实验四：神经网络反向传播算法](#ex4)  



<!-- more -->

<a NAME="ex1"></a>

## 实验一，Programming Assignment: Linear Regression #

这个实验就是用来熟悉一下提交的环境，另外在学习算法的各个过程都需要填几行关键程序，包括怎么求估价函数，怎么施行阶梯算法，怎么标准化数据等等，选做实验是两种特征的训练数据。

### 1.1 单位矩阵 #
就是通过eye函数得一个单位矩阵。

### 1.2 计算估价函数J #
（对不起我还不会写公式，回来更新）


	J = sum((X * theta - y).^2) / (2*m);


注意其中的.^，这是矩阵特有的操作符号，是矩阵的每一个对应的位置进行相应的运算。sum函数则是对矩阵求和。如果得出的结果是32.07就说明程序写的没问题，总之矩阵运算一定要注意的是x*y对应起来，否则如果出错还容易发现矩阵运算没有写错，如果恰好没有提示错误，得出的结果不对这种Bug会改的头破血流。

### 1.3 实现阶梯下降算法 #
（公式仍然不会写）

	theta(1) = theta(1) - alpha/m*sum(X*theta_backup-y);
	theta(2) = theta(2) - alpha/m*sum((X*theta_backup-y).*X(:,2));

这只是单纯照着阶梯下降算法更新theta就好。值得注意的地方在上课的时候NG也说了，因为再更新theta的过程中会改变theta，所以我这里用了一个 ** theta_backup ** 对原来的theta进行了备份。这是一个特征的情况，下面还会有多种特征的简便写法（我第一次写的是循环^o^）。


这个实验做到这里其实已差不多了，看到最后画出的图还是很开心的，虽然感觉上跟我并没有什么关系。

### 1.4 多类特征的附加实验 #
具体步骤跟单类特征差不多，值得注意的就是上文提到的更新theta的简便写法：
	
	theta = theta - alpha / m * X' * (X * theta - y); 

X'是求X的转置矩阵。

这是对数据标准化的语句，话说在pdf中间直接告诉要怎么做这样真的好嘛？连需要用的函数都给了。。。

	mu = mean(X);
	sigma = std(X);
	X_norm = (X - repmat(mu, size(X,1) , 1))  ./ repmat(sigma,size(X,1),1);

mean()函数是求平均值，具体自己做做实验就知道了，我就是一边儿一边儿在shell上面测试函数的用法，实用是学习最快的方法。std()函数是求标准差，然后完全按照pdf的描述就可以写下这条语句了（当然，我这么笨的人在shell上面测试了好久。。）。

另外视频中也提到过，如果数据规模小的话，可以直接使用矩阵进行计算：

	theta = pinv(X'*X)*X'*y;

pinv()函数是求矩阵的逆，其实就是这个函数限制着这种干脆利索的方法的运行速度的。

### 1.5 总结 #
总之，第一次实验叫做 **worm exercises** 也是有一定道理的。其实就是给 **纸上得来** 和 **恭行** 两件事建立个联系，知道这些算法是可以实现并且实际工作的，这对于我这样的笨蛋还是很重要的，学的时候总觉得隔层纱，到真的程序运行结果砸在脸上了，就能迷糊回来了。

<a NAME="ex2"></a>

## 实验二，Logistic Regression #

该实验总共包含两个小实验，一个是基础的逻辑回归实验，另一个是情况稍微复杂一点的实验，需要拟合的数据不是直线，当然其实如果没想那么多，跟着pdf做的话，几乎感觉不到区别，除了最后可视化的图不太一样。

### 2.1 将数据可视化 #

可以看到，边界线大概是条直线。

### 2.2 实现 #

经过前面的实验熏陶，已经知道这个实验实现部分肯定包括CostFunction，下降率，自己对数据进行预测等等，所有需要做的就是照着公式用octave实现出来就可，再次强调 **矩阵的行和列** 是验算式子写的对不对的有效的方法。
sigmoid函数很简单，不过注意的是这个函数接受的参数z，是矩阵，所以使用操作符号的时候一定要注意，该用“./”就要用。
求J的CostFunction，解析起来比较复杂，还是惯例，从里到外解析，注意运算符左右的矩阵的行和列，一般不会有什么问题，具体公式如下所示：

	J = 1 / m * sum((-y).*log(sigmoid(X*theta)) - (1-y).*log(1-sigmoid(X*theta)));

要注意的还是诸如“.*”这些操作符号。
而相关的gradient更新的公式如下：

	grad = 1 / m * (X' * (sigmoid(X*theta) - y));

---
其实costfunction写完之后，需要修改的文件已经差不多没了，但是任务并没有结束，有了costfunction要如何更新theta呢？
这里是使用了octave里面提供的fminunc函数，通过这个函数算出对于CostFunction来说最优的参数。使用方式如下所示：

	options = optimset('GradObj', 'on', 'MaxIter', 400);
	[theta, cost] = fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);

GradObj参数告诉fminunc使用gradient来计算需要返回theta和cost两个值，设置MaxIter最大步数为400。后面@(t)的写法是内部调用函数，同时提供theta和上文提到的options就可以等待fminunc算出结果了。并不用自己制定下降率之类的参数，大部分工作都由fminunc做了。

### 2.3 评估结果 #

这里就是将已经得到的theta带进h函数就好，然后把所有的值在0.5以上的数据挑出来，将其y置1就好，使用find，很容易达到目的。

	tmp = sigmoid(X*theta);
	finde = find(tmp>=0.5);
	p(finde,1) = 1;

然后就可以看到结果啦。

### 2.4 第二个实验：Regularized logistic regression

整个流程和第一个实验差不多，需要注意的是costFunction中计算J函数和gradient更新，对于j=0和j>0是不一样的处理方式，另外在mapFeature函数中，使用了6次的多项式对数据进行拟合，可以看其拟合的手法，值得学习。
最后还给出了过度拟合与拟合度不够的边界值的图，可以看看。

### 2.5 总结 #

整体来说，大部分程序不需要自己动手，只是部分函数自己填一下就可以了，像填空一样，但是做到现在，应该慢慢对整个体系有一些印象，回来是需要自己把整个过程都写出来的，提前做准备。


<a NAME="ex3"></a>

## 实验三，神经网络算法 #

这个实验使用了视频最后讲的识别手写数字作为数据输入，分别使用了多种特征的逻辑回归算法与神经网络算法进行实验，用来加强这两种算法的对比。实际运行时也会有个大概的印象，在特征较多时**神经网络算法果然比逻辑回归要快**。

### 3.1 多重特征的逻辑回归算法 #

这一小节就是对上一次实验的复习，如果上次实验costFunction和grad写的好的话，这次可以直接拿来用。但是果然还是再做一边比较好。
然后调用fmincg函数，比之前用的fminunc在处理大规模数据方面更有效率。
但是使用方法和fminunc几乎一样。
然后根据前面训练出来的theta对手写数字进行预测，一切都差不多。需要注意的就是这里使用max函数，得出的是每行最大的值（越大越正确），需要用一个两列的矩阵接受max函数的返回值，这样，第一列返回的是最大值，第二列返回的是最大值所在的位置。第二列才是我们需要的。第几个最大，预测就是几。

### 3.2 神经网络算法 #

所有的theta已经算好，而且模型也给出了，是一个具有一个hidden层的模型，hidden层具有25个单元，输出层有10个单元。
那由于数据是有4000个特征的。所以可以预测theta1是25×401的矩阵，而theta2是10×26的矩阵。
于是有了theta之后，就可以一层一层的进行计算了，每层记得加上x0。
如下所示：

	tmpX = [ones(m,1) X];
	layer1 = sigmoid(tmpX * Theta1');
	tmplayer1 = [ones(m,1) layer1];
	layer2 = sigmoid(tmplayer1 * Theta2');
	[a,p] = max(layer2,[],2);

代码很简单，最后的max函数上面也讲过了，需要用一个两列的矩阵接受返回值，第二列就是我们的预测值。（他把10当作0了）

<a NAME="ex4"></a>  

## 实验四 神经网络与反向传播算法 #

本次实验主要实现了包括反向传播算法在内的比较完整的神经网络学习算法，由于这个算法比较复杂，实现起来步骤也比较多，所以经常容易出错，甚至这个算法后面还花了很大的篇幅给了一个校验算法，可能都觉得这算法偷摸在后台跑着很不让程序员放心。
本次实验将神经网络学习算法分为了两个部分，第一是前向传播，这部分和上个实验的相关部分相近，神经网络模型也是三层，Theta1是25 × 401，Theta2是10 × 26。在之后写代码的时候会经常遇到。

### 4.1 前向算法的实现 #

具体步骤和实验三类似，但是需要注意的是，这里输出层包含了十个单元，但是给出的y只是1-10的数字，需要将y转成位图存储，具体代码如下：

	Y = [];
	E = eye(num_labels);
	for i=1:num_labels
		Y0 = find(y==i);
		Y(Y0,:) = repmat(E(i,:),size(Y0,1),1);
	end

主要用了find函数与repmat函数，find函数前面有介绍，而repmat函数就是重复矩阵，所以这里又要说了，矩阵运算赛高～～

这里的costFunction看起来非常复杂，需要算有三个求和符号的式子，而且就算是向量算式仍然还有两个求和符号，善用sum求和函数即可，复杂但是不难。对了，记得加上正则化的参数。

learning部分，包括梯度导数都将在下一节介绍。

### 4.2 后向传播算法的实现 #
这一次实验的重点来了，梯度导数算起来步骤繁多，还是善用解构的方法，现处理和别的联系不大的模块，官方给的pdf也是先要处理sigmoid导数计算的小模块，pdf中已经给出了导数的式子，没事儿可以自己求个导～
将随机初始化处理了之后，就开始本次实验的重头戏，后向传播算法。官方pdf中显然也是非常不放心将步骤这么多的一个算法交给我们这些小朋友，所以步骤都给清楚了，而我在做的时候，也是老老实实重新看了一边视频，将重点重新复习了一下才开始动笔写的。好了严格按照步骤写就好。

**step 1**  
求a1，我们知道a1就是X中某一个样本特征，但是记得加上bias。由于要算偏差值和delta，所以需要再进行一边前向传播，分别算出a2，a3，z2，z3。

**step 2**  
算偏差值，err3单独算，然后按照给的公式一步一步算出err2就好。这里给出err2的计算式子。

	err_2 = (Theta2' * err_3);
	err_2 = err_2(2:end) .* sigmoidGradient(z_2);

这里切记切记，z2是25 × 1，而err2是26 × 1，需要将bias干掉再算，当然如果在写代码的时候时刻记着现在矩阵计算的行和列，那就不会像我一样犯错误了。

**step 3**  
计算delta1和delta2。按照公式计算即可。

	delta_2 = delta_2 + (err_3)*a_2';
	delta_1 = delta_1 + (err_2)*a_1';

**step 4**
计算两个梯度导数，还是一样按照公式计算即可。

	Theta1_grad = 1 / m * delta_1 + lambda / m * theta1_tmp;
	Theta2_grad = 1 / m * delta_2 + lambda / m * theta2_tmp;

其实按照步骤分解，每一步视频都讲的挺清楚，注意代码编写正确即可。

之后官方已经给了校验算法，自己跑一遍ex4就行。

### 4.3 关于隐藏层的可视化 #

这个额外的实验就一个问题：隐藏层究竟代表了什么？
其实咱们小时候学习的时候也是这样，学写字，大概长这个样子就是什么字儿，大概是这个样子是什么句子（还记得那个文字顺序一点儿都不影响阅读么？）。原来在看名家在争论白马非马的时候就跟着思考过这个概念，就是**大概长这个样子**用不是人话描述就叫模型，所谓马为天下之马而白马就这一匹而已。之所以这个算法叫神经网络算法，其实也是这样，每一个中间层可能就是对某一类事物的模型，每层递进的过程中都在一点儿一点儿完善这个模型。

实验一更新（2015.05.14 21：28）  
实验二更新（2015.05.20 09：50）  
实验三更新（2015.05.27 09：23）  
实验四更新（2015.06.09 09：09）  
