---
layout: post
title: NG机器学习的编程实验
category: 机器学习
date: 2015-05-14
---

[实验一：线性回归](#ex1)

今天把NG的机器学习公开课的作业又顺了一边，很多语句不用就忘了，另外矩阵运算用起来就是让人*心旷神怡*。

<!-- more -->

<a NAME="ex1"></a>    

## 实验一，Programming Assignment: Linear Regression #

这个实验就是用来熟悉一下提交的环境，另外在学习算法的各个过程都需要填几行关键程序，包括怎么求估价函数，怎么施行阶梯算法，怎么标准化数据等等，选做实验是两种特征的训练数据。

### 1.1 单位矩阵 #
就是通过eye函数得一个单位矩阵。

### 1.2 计算估价函数J #
（对不起我还不会写公式，回来更新）


	J = sum((X * theta - y).^2) / (2*m);


注意其中的.^，这是矩阵特有的操作符号，是矩阵的每一个对应的位置进行相应的运算。sum函数则是对矩阵求和。如果得出的结果是32.07就说明程序写的没问题，总之矩阵运算一定要注意的是x*y对应起来，否则如果出错还容易发现矩阵运算没有写错，如果恰好没有提示错误，得出的结果不对这种Bug会改的头破血流。

### 1.3 实现阶梯下降算法 #
（公式仍然不会写）

	theta(1) = theta(1) - alpha/m*sum(X*theta_backup-y);
	theta(2) = theta(2) - alpha/m*sum((X*theta_backup-y).*X(:,2));

这只是单纯照着阶梯下降算法更新theta就好。值得注意的地方在上课的时候NG也说了，因为再更新theta的过程中会改变theta，所以我这里用了一个 ** theta_backup ** 对原来的theta进行了备份。这是一个特征的情况，下面还会有多种特征的简便写法（我第一次写的是循环^o^）。


这个实验做到这里其实已差不多了，看到最后画出的图还是很开心的，虽然感觉上跟我并没有什么关系。

### 1.4 多类特征的附加实验 #
具体步骤跟单类特征差不多，值得注意的就是上文提到的更新theta的简便写法：
	
	theta = theta - alpha / m * X' * (X * theta - y); 

X'是求X的转置矩阵。

这是对数据标准化的语句，话说在pdf中间直接告诉要怎么做这样真的好嘛？连需要用的函数都给了。。。

	mu = mean(X);
	sigma = std(X);
	X_norm = (X - repmat(mu, size(X,1) , 1))  ./ repmat(sigma,size(X,1),1);

mean()函数是求平均值，具体自己做做实验就知道了，我就是一边儿一边儿在shell上面测试函数的用法，实用是学习最快的方法。std()函数是求标准差，然后完全按照pdf的描述就可以写下这条语句了（当然，我这么笨的人在shell上面测试了好久。。）。

另外视频中也提到过，如果数据规模小的话，可以直接使用矩阵进行计算：

	theta = pinv(X'*X)*X'*y;

pinv()函数是求矩阵的逆，其实就是这个函数限制着这种干脆利索的方法的运行速度的。

### 1.5 总结 #
总之，第一次实验叫做 **worm exercises** 也是有一定道理的。其实就是给 **纸上得来** 和 **恭行** 两件事建立个联系，知道这些算法是可以实现并且实际工作的，这对于我这样的笨蛋还是很重要的，学的时候总觉得隔层纱，到真的程序运行结果砸在脸上了，就能迷糊回来了。

（2015.05.14 21：28）
